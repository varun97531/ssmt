{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cbfccfe815614271a0354c589988316b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a5f3a82550d4d79b80416b36b17e97f",
              "IPY_MODEL_d8d1be74722248a09b8bdf447007025e",
              "IPY_MODEL_43c3da295bdd4fc7a0c19e940bb7d4f9"
            ],
            "layout": "IPY_MODEL_35d8da9d5fb643e0ab279cd2f89db3f2"
          }
        },
        "5a5f3a82550d4d79b80416b36b17e97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f67e0c6e58b4267bc3a7c9c7072dc20",
            "placeholder": "​",
            "style": "IPY_MODEL_5acbd0303dd74029909815286074490f",
            "value": "Downloading readme: 100%"
          }
        },
        "d8d1be74722248a09b8bdf447007025e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cef1cd78453144a4922d03ac495c90f5",
            "max": 3140,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ea9408932b54245974015634bf4d98b",
            "value": 3140
          }
        },
        "43c3da295bdd4fc7a0c19e940bb7d4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b867d1ffbd964dcdb95a3b4f0cde2600",
            "placeholder": "​",
            "style": "IPY_MODEL_31a04ca7ca904e5c90d3fcf95a1e6578",
            "value": " 3.14k/3.14k [00:00&lt;00:00, 177kB/s]"
          }
        },
        "35d8da9d5fb643e0ab279cd2f89db3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f67e0c6e58b4267bc3a7c9c7072dc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5acbd0303dd74029909815286074490f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cef1cd78453144a4922d03ac495c90f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea9408932b54245974015634bf4d98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b867d1ffbd964dcdb95a3b4f0cde2600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a04ca7ca904e5c90d3fcf95a1e6578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ba0a9dd73f24a6994814470f047c545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18ea26b1d471491090064cbf10868689",
              "IPY_MODEL_1d709171e19747f586962aeeb51113ae",
              "IPY_MODEL_c7320d5f4b66428b9c7301d07929f812"
            ],
            "layout": "IPY_MODEL_5ea375fe22a4448ab46e1d9b54abf25b"
          }
        },
        "18ea26b1d471491090064cbf10868689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8162a4b77f7540cbad4cbaa599018957",
            "placeholder": "​",
            "style": "IPY_MODEL_0c425dc43acf4f44a4890ff93b235718",
            "value": "Downloading metadata: 100%"
          }
        },
        "1d709171e19747f586962aeeb51113ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59272b8e7b4b4938b589a980c7ba0f51",
            "max": 953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcf95a811fb44d82985bd533b6043156",
            "value": 953
          }
        },
        "c7320d5f4b66428b9c7301d07929f812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6874123f66c46d0b116410a25332eb0",
            "placeholder": "​",
            "style": "IPY_MODEL_c041d6b3ed0a4e39befec67b87c36b3c",
            "value": " 953/953 [00:00&lt;00:00, 66.6kB/s]"
          }
        },
        "5ea375fe22a4448ab46e1d9b54abf25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8162a4b77f7540cbad4cbaa599018957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c425dc43acf4f44a4890ff93b235718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59272b8e7b4b4938b589a980c7ba0f51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf95a811fb44d82985bd533b6043156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6874123f66c46d0b116410a25332eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c041d6b3ed0a4e39befec67b87c36b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4845dc77161f4e84af2e05a219311f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_feebb83b6f7847d5ac3e52046936577d",
              "IPY_MODEL_a5b452eb4a414023802a8dd28509b2f9",
              "IPY_MODEL_5aa25f5eee62409eb1a02b84770d142a"
            ],
            "layout": "IPY_MODEL_d28cdee519134278ac5b4521452f4e99"
          }
        },
        "feebb83b6f7847d5ac3e52046936577d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_611c43b7b2ae4ea3ae642cd65bb29c4c",
            "placeholder": "​",
            "style": "IPY_MODEL_63ac8d066ba74c6f8ba6dc025e221b9e",
            "value": "Downloading data: 100%"
          }
        },
        "a5b452eb4a414023802a8dd28509b2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b0bc7b3f751446ba0497deaf7c99578",
            "max": 189602204,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57078adc703441ef8425beeed6e5114e",
            "value": 189602204
          }
        },
        "5aa25f5eee62409eb1a02b84770d142a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865706fe5cae4b5ebdf22e3182030d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_6123694393854a15889420ea90f4ce38",
            "value": " 190M/190M [00:01&lt;00:00, 249MB/s]"
          }
        },
        "d28cdee519134278ac5b4521452f4e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "611c43b7b2ae4ea3ae642cd65bb29c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ac8d066ba74c6f8ba6dc025e221b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b0bc7b3f751446ba0497deaf7c99578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57078adc703441ef8425beeed6e5114e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "865706fe5cae4b5ebdf22e3182030d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6123694393854a15889420ea90f4ce38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "293f8c58c60442968ca4e10aea19cdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b5814c554844f7598d8f8101b38190f",
              "IPY_MODEL_e78a0a85086f42be8ff2fb7f0140614f",
              "IPY_MODEL_5b8c00cd03184993b1d071a61dce1dd5"
            ],
            "layout": "IPY_MODEL_fa27b76cb41349fe97f11fbd3fd05bf5"
          }
        },
        "5b5814c554844f7598d8f8101b38190f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_455b61902584403b8186b55f99230529",
            "placeholder": "​",
            "style": "IPY_MODEL_879d74172cb045eb9667a35820c427e9",
            "value": "Downloading data: 100%"
          }
        },
        "e78a0a85086f42be8ff2fb7f0140614f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94e78cdc3b214fb7804b48878caeb04a",
            "max": 85707,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d3a66c81d5447919b75234553e9e61f",
            "value": 85707
          }
        },
        "5b8c00cd03184993b1d071a61dce1dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43d2fcbbe7454d70a5b4648548902e7f",
            "placeholder": "​",
            "style": "IPY_MODEL_66b6ac687dab483a96867656565f61eb",
            "value": " 85.7k/85.7k [00:00&lt;00:00, 324kB/s]"
          }
        },
        "fa27b76cb41349fe97f11fbd3fd05bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455b61902584403b8186b55f99230529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "879d74172cb045eb9667a35820c427e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94e78cdc3b214fb7804b48878caeb04a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d3a66c81d5447919b75234553e9e61f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43d2fcbbe7454d70a5b4648548902e7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b6ac687dab483a96867656565f61eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "361df7593fc447eb87c2ab4af6ec5975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84ef6e5cea5347f4bc4e8357176f8581",
              "IPY_MODEL_7805c898692b4872a9f8a88ffa6b5c64",
              "IPY_MODEL_d3738b34fd1c4f45ad7d03feb1431072"
            ],
            "layout": "IPY_MODEL_9f1d9ea605b640bbb701bb3106e877e1"
          }
        },
        "84ef6e5cea5347f4bc4e8357176f8581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d075f9d669b498ab5e4c700dfd62397",
            "placeholder": "​",
            "style": "IPY_MODEL_485ad9006d8c4f69b09d59a47c7322d3",
            "value": "Downloading data: 100%"
          }
        },
        "7805c898692b4872a9f8a88ffa6b5c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1809fe39fa4b6bbe58b6f6181549b1",
            "max": 500080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9428d095398c48b583c1fe9860881ce4",
            "value": 500080
          }
        },
        "d3738b34fd1c4f45ad7d03feb1431072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb468b2793664e57b6e5520f1e68e390",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf3cc7fb90642c88851c6f114f23a9e",
            "value": " 500k/500k [00:00&lt;00:00, 2.48MB/s]"
          }
        },
        "9f1d9ea605b640bbb701bb3106e877e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d075f9d669b498ab5e4c700dfd62397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485ad9006d8c4f69b09d59a47c7322d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b1809fe39fa4b6bbe58b6f6181549b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9428d095398c48b583c1fe9860881ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb468b2793664e57b6e5520f1e68e390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf3cc7fb90642c88851c6f114f23a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0edc1e3af741445dbd7c7375b66a3358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67c4c81f35744801b904f68874034a39",
              "IPY_MODEL_cd4e5ca2a7eb4de985db85f1bc007f75",
              "IPY_MODEL_8ef6780b995d4821a9617820b840cc74"
            ],
            "layout": "IPY_MODEL_bc6be9d30fd740ebbd8f7bbd2b384d5f"
          }
        },
        "67c4c81f35744801b904f68874034a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_989bc6a41a4c4938ae647e721b299365",
            "placeholder": "​",
            "style": "IPY_MODEL_cb8b25f64b3c442faca5f8d758797528",
            "value": "Generating train split: 100%"
          }
        },
        "cd4e5ca2a7eb4de985db85f1bc007f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3687cdea9a7247dea956d9ab6d103282",
            "max": 1659083,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7798b592f25f4b4b8f3e008a94838bde",
            "value": 1659083
          }
        },
        "8ef6780b995d4821a9617820b840cc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83a3d05ed09543719737b1778feff09e",
            "placeholder": "​",
            "style": "IPY_MODEL_7b38bead326042a6a6dd200001b3eb50",
            "value": " 1659083/1659083 [00:04&lt;00:00, 332383.87 examples/s]"
          }
        },
        "bc6be9d30fd740ebbd8f7bbd2b384d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "989bc6a41a4c4938ae647e721b299365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8b25f64b3c442faca5f8d758797528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3687cdea9a7247dea956d9ab6d103282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7798b592f25f4b4b8f3e008a94838bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83a3d05ed09543719737b1778feff09e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b38bead326042a6a6dd200001b3eb50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef4bfd622fbb4dffb1933410d477a402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6460f75a98c843bf94f5a9d233b2a27b",
              "IPY_MODEL_b599371194134604b11b35b456084538",
              "IPY_MODEL_95f45918a6744d1ba7604eaf9d071eef"
            ],
            "layout": "IPY_MODEL_fbce4086ade248d6a2af896cb91aaf4c"
          }
        },
        "6460f75a98c843bf94f5a9d233b2a27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf4e42d6f3a941aab7c0fbe151e89989",
            "placeholder": "​",
            "style": "IPY_MODEL_a51a7c7dc42c4a6cb6043fcf6714ae69",
            "value": "Generating validation split: 100%"
          }
        },
        "b599371194134604b11b35b456084538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7711c1113a804f64afee5ce9f5af72ab",
            "max": 520,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8746dc7b112a4bc3bdef99fc8ae8dcf2",
            "value": 520
          }
        },
        "95f45918a6744d1ba7604eaf9d071eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12b3373c30a4d43acc668a21e3f3cad",
            "placeholder": "​",
            "style": "IPY_MODEL_41adb8a9a9cb40ee9d5e96d6847d40ed",
            "value": " 520/520 [00:00&lt;00:00, 9507.54 examples/s]"
          }
        },
        "fbce4086ade248d6a2af896cb91aaf4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf4e42d6f3a941aab7c0fbe151e89989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a51a7c7dc42c4a6cb6043fcf6714ae69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7711c1113a804f64afee5ce9f5af72ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8746dc7b112a4bc3bdef99fc8ae8dcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c12b3373c30a4d43acc668a21e3f3cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41adb8a9a9cb40ee9d5e96d6847d40ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d1a97338977487fa382fee83afb4500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ea3f28827bb4c77912089d1c8b83c65",
              "IPY_MODEL_0364d83e00854ea988da44153729bd7d",
              "IPY_MODEL_7b25913d86f74c39bb15189b4bbeee62"
            ],
            "layout": "IPY_MODEL_ad8caa4a0adf433aa6b940d4b214cdbb"
          }
        },
        "1ea3f28827bb4c77912089d1c8b83c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beaa52119428406ebcf2fb03350bea0b",
            "placeholder": "​",
            "style": "IPY_MODEL_5611c24a255246a186495ea6803dde0f",
            "value": "Generating test split: 100%"
          }
        },
        "0364d83e00854ea988da44153729bd7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2875cdd66534990a28fb502356481b7",
            "max": 2507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ca832b97a644c3bbd9775704d74e002",
            "value": 2507
          }
        },
        "7b25913d86f74c39bb15189b4bbeee62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8310ba66509492b90605acb2e4f599c",
            "placeholder": "​",
            "style": "IPY_MODEL_02dbe1d661c54b6ca0e77660ce861178",
            "value": " 2507/2507 [00:00&lt;00:00, 42679.16 examples/s]"
          }
        },
        "ad8caa4a0adf433aa6b940d4b214cdbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beaa52119428406ebcf2fb03350bea0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5611c24a255246a186495ea6803dde0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2875cdd66534990a28fb502356481b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca832b97a644c3bbd9775704d74e002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8310ba66509492b90605acb2e4f599c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02dbe1d661c54b6ca0e77660ce861178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XW6dP3jxTTh0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "# from model_utility.data_prep_utils import remove_sc, clean_text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "content = \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNHLrxJ-T0w4",
        "outputId": "7a89bbb4-2c8f-45ee-fc99-97433ffdb498"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r \"/content/drive/MyDrive/ssmt/requirement.txt\"\n"
      ],
      "metadata": {
        "id": "Vs3oaIXdXNdX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to the English folder\n",
        "eng_folder_path = os.path.join(content, \"ssmt\", \"Eng_all\", \"Eng_all\")\n",
        "\n",
        "# List all files in the English folder\n",
        "eng_files = glob.glob(os.path.join(eng_folder_path, \"*\"))\n",
        "\n",
        "# Path to the Hindi folder\n",
        "hin_folder_path = os.path.join(content, \"ssmt\", \"Hin_all\", \"Hin_all\")\n",
        "\n",
        "# List all files in the Hindi folder\n",
        "hin_files = glob.glob(os.path.join(hin_folder_path, \"*\"))\n"
      ],
      "metadata": {
        "id": "A6_M5oRBUlLF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "eng_directory = '/content/drive/MyDrive/ssmt/Eng_all/Eng_all/'\n",
        "eng_files = os.listdir(eng_directory)\n",
        "\n",
        "\n",
        "hin_directory = '/content/drive/MyDrive/ssmt/Hin_all/Hin_all/'\n",
        "hin_files = os.listdir(hin_directory)"
      ],
      "metadata": {
        "id": "g6WQWarGU4QQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1scMUiwU-Gp",
        "outputId": "af6c419a-e9ed-4dfb-a9a7-0f0bde6547ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['control flow [oe6iF3yzMo8].en.txt',\n",
              " 'euclid_s algorithm for gcd [TURlDFwVeEs].en.txt',\n",
              " 'classes and objects in python [KhvV7EEEq4I].en.txt',\n",
              " 'arrays vs lists, binary search [0y5HOotxpys].en.txt',\n",
              " 'downloading and installing python [Muzbm6FFS90].en.txt',\n",
              " 'efficiency [a87JSpv4r44].en.txt',\n",
              " 'algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt',\n",
              " 'backtracking, n queens [kdBzkxdJ7bI].en.txt',\n",
              " 'breaking out of a loop [ucjfYt6Q9Bo].en.txt',\n",
              " 'memoization and dynamic programming [h_YCj6CeutA].en.txt',\n",
              " 'formatting printed output [LdEEkFL1Kvg].en.txt',\n",
              " 'mergesort, analysis [JusPS-aLd04].en.txt',\n",
              " 'grid paths [SD-hwfZykDM].en.txt',\n",
              " 'assignment statement, basic types - int, float, bool [CeHWTBQpiFE].en.txt',\n",
              " 'manipulating lists [lOhshXnJAJw].en.txt',\n",
              " 'list comprehension [Uu4PnnWlqsA].en.txt',\n",
              " 'generating permutations [rP9XZRGPvdQ].en.txt',\n",
              " 'handling files [FWB0t6TcH3E].en.txt',\n",
              " 'improving naive gcd [Bc__4fV94lE].en.txt',\n",
              " 'functions [HGGdN94SvC8].en.txt',\n",
              " 'search trees [JEqF-kIzYl8].en.txt',\n",
              " 'lists [ffMgNo17Ork].en.txt',\n",
              " 'recursion [TAOziHOk7iA].en.txt',\n",
              " 'priority queues and heaps [Npjk0qMfOM8].en.txt',\n",
              " 'quicksort analysis [uRmg6Jl0mYk].en.txt',\n",
              " 'user defined lists [mK7u8wf6pMs].en.txt',\n",
              " 'more about range() [ZgEB1oCzVnY].en.txt',\n",
              " 'sets, stacks, queues [l_5bPOV7qB8].en.txt',\n",
              " 'insertion sort [QnjToZfpi0E].en.txt',\n",
              " 'strings [I7zOYKF4AHc].en.txt',\n",
              " 'wrap-up, python vs other languages [dXVzBXhyMxY].en.txt',\n",
              " 'standard input and output [2DAYQeWS7Mw].en.txt',\n",
              " 'function definitions [2aidDiaEst0].en.txt',\n",
              " 'string functions [naRMZBvRLhg].en.txt',\n",
              " 'longest common subsequence [P-oDU7AQ8Mo].en.txt',\n",
              " 'exception handling [RcTb2kpqTsg].en.txt',\n",
              " 'global scope, nested functions [v2MUlo-JuT4].en.txt',\n",
              " 'quicksort [zjqzrcljMlI].en.txt',\n",
              " 'tuples and dictionaries [lR8DWx2fcbQ].en.txt',\n",
              " 'mergesort [V7fvTmhqokM].en.txt',\n",
              " 'examples [J_Cw3G5v460].en.txt',\n",
              " 'matrix multiplication [1iaItgJ3zLg].en.txt',\n",
              " 'selection sort [BruMUw6mV1c].en.txt',\n",
              " 'pass, del() and none [7IwCDrIasps].en.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content of each English file, convert to lowercase, and strip whitespace\n",
        "eng_file_contents = {}\n",
        "hin_file_contents = {}\n",
        "\n",
        "for file_name in hin_files:\n",
        "    eng_file_path = os.path.join(eng_directory, file_name)\n",
        "    hin_file_path = os.path.join(hin_directory, file_name)\n",
        "    if os.path.isfile(eng_file_path):\n",
        "        with open(eng_file_path, 'r') as file:\n",
        "            lines = [line.lower().strip() for line in file.readlines()]\n",
        "            eng_file_contents[file_name] = lines\n",
        "\n",
        "    if os.path.isfile(hin_file_path):\n",
        "        with open(hin_file_path, 'r') as file:\n",
        "            lines = [line.lower().strip() for line in file.readlines()]\n",
        "            hin_file_contents[file_name] = lines"
      ],
      "metadata": {
        "id": "OogExDC4U-EG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for val in eng_file_contents.keys():\n",
        "    print(val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MemU-LVbU-Bk",
        "outputId": "ae9fa202-13a7-47eb-dba5-4947af1f96b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes and objects in python [KhvV7EEEq4I].en.txt\n",
            "algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt\n",
            "downloading and installing python [Muzbm6FFS90].en.txt\n",
            "euclid_s algorithm for gcd [TURlDFwVeEs].en.txt\n",
            "efficiency [a87JSpv4r44].en.txt\n",
            "breaking out of a loop [ucjfYt6Q9Bo].en.txt\n",
            "function definitions [2aidDiaEst0].en.txt\n",
            "grid paths [SD-hwfZykDM].en.txt\n",
            "examples [J_Cw3G5v460].en.txt\n",
            "assignment statement, basic types - int, float, bool [CeHWTBQpiFE].en.txt\n",
            "control flow [oe6iF3yzMo8].en.txt\n",
            "improving naive gcd [Bc__4fV94lE].en.txt\n",
            "backtracking, n queens [kdBzkxdJ7bI].en.txt\n",
            "functions [HGGdN94SvC8].en.txt\n",
            "manipulating lists [lOhshXnJAJw].en.txt\n",
            "lists [ffMgNo17Ork].en.txt\n",
            "generating permutations [rP9XZRGPvdQ].en.txt\n",
            "memoization and dynamic programming [h_YCj6CeutA].en.txt\n",
            "handling files [FWB0t6TcH3E].en.txt\n",
            "insertion sort [QnjToZfpi0E].en.txt\n",
            "list comprehension [Uu4PnnWlqsA].en.txt\n",
            "arrays vs lists, binary search [0y5HOotxpys].en.txt\n",
            "longest common subsequence [P-oDU7AQ8Mo].en.txt\n",
            "exception handling [RcTb2kpqTsg].en.txt\n",
            "mergesort, analysis [JusPS-aLd04].en.txt\n",
            "search trees [JEqF-kIzYl8].en.txt\n",
            "quicksort analysis [uRmg6Jl0mYk].en.txt\n",
            "more about range() [ZgEB1oCzVnY].en.txt\n",
            "tuples and dictionaries [lR8DWx2fcbQ].en.txt\n",
            "global scope, nested functions [v2MUlo-JuT4].en.txt\n",
            "user defined lists [mK7u8wf6pMs].en.txt\n",
            "formatting printed output [LdEEkFL1Kvg].en.txt\n",
            "recursion [TAOziHOk7iA].en.txt\n",
            "selection sort [BruMUw6mV1c].en.txt\n",
            "mergesort [V7fvTmhqokM].en.txt\n",
            "priority queues and heaps [Npjk0qMfOM8].en.txt\n",
            "strings [I7zOYKF4AHc].en.txt\n",
            "string functions [naRMZBvRLhg].en.txt\n",
            "quicksort [zjqzrcljMlI].en.txt\n",
            "standard input and output [2DAYQeWS7Mw].en.txt\n",
            "wrap-up, python vs other languages [dXVzBXhyMxY].en.txt\n",
            "sets, stacks, queues [l_5bPOV7qB8].en.txt\n",
            "pass, del() and none [7IwCDrIasps].en.txt\n",
            "matrix multiplication [1iaItgJ3zLg].en.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_file_contents['control flow [oe6iF3yzMo8].en.txt']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWxM6wDtU9_K",
        "outputId": "0e9a94d1-90b6-4126-dbbc-522e30902075"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in the past few lectures, we have looked at values of different types in python, starting   with the numbers and then moving onto the booleans and then sequences such as strings   and lists.',\n",
              " 'now let us go back to the order in which statements are executed.',\n",
              " 'remember we said that a typical python program will have a collection of function definitions   followed by a bunch of statements.',\n",
              " 'in general, the python interpreter will read whatever we give it from top to bottom.',\n",
              " 'so, when it sees the definition of a function, it will digest it but not execute it.',\n",
              " 'we will look at function definitions in more detail very soon.',\n",
              " 'and when we now come to something which is not a definition then python will try to execute   it, this in turn could involve invoking a function in which case the statements which   define the function will be executed and so on.',\n",
              " 'however, if we have this kind of a rigid straight-line execution of our program then we can only   do limited things.',\n",
              " 'this means we have an inflexible sequence of steps that we always follow regardless   of what we want to do.',\n",
              " 'now in many situations, many realistic situations, we need to vary the next step according to   what has happened so far, according to the current values that we see.',\n",
              " 'let us look at a real life example.',\n",
              " 'supposing, you are packing your things to leave for the bus stop in the morning, or   whether or not you take an umbrella with you will depend on whether you think it is going   to rain that day.',\n",
              " 'if you carry the umbrella all the time then your bag becomes heavy, if you do not carry   the umbrella ever, then you risk the chance of being wet.',\n",
              " 'so, you would stop at this point and see, in whatever way by reading the weather forecast   or looking out of the window is it likely to rain today?   if it is likely to rain, ensure that the umbrellas in your bag, put it if it is not there, or   leave it if it is already there.',\n",
              " 'if it is not likely to rain, ensure the umbrella is not in the bag, if it is not there it is   fine otherwise, take it out.',\n",
              " 'so, this kind of execution which varies the order in which statements are executed is   referred to in programming languages as control flow.',\n",
              " 'there are three fundamental things all of which we have see informally in the gcd case.',\n",
              " 'one is what we just describe the conditional execution.',\n",
              " 'the other is when we want to repeat something a fixed number of times and this number of   times is known in advance.',\n",
              " 'we want to carry 10 boxes from this room to that room, so 10 times we carry one box at   a time from here to there.',\n",
              " 'on the other hand, sometime we may want to repeat something where the number of repetitions   is not known in advance.',\n",
              " 'suppose, we put sugar in our tea cup and we want to stir it till the sugar dissolves.',\n",
              " 'so, what we will do, we stir it once check, if there is still sugar stir it again, check   it there is still sugar and so on, and as the sugar dissolves finally after one round   we will find there is no sugar at the bottom of the cup and we will stop stirring.',\n",
              " 'here, we will repeat the stirring action a number of times, but we will not know in advance   whether we have to stir it twice or five times, we will stir until the sugar dissolves.',\n",
              " 'let us begin with conditional execution.',\n",
              " \"conditional execution in python is written as we saw using the 'if statement'.\",\n",
              " \"we have 'if', then we have a conditional expression which returns a value true or false typically   a comparison, but it could also be invoking a function which returns true or false for   example, and we have this colon which indicates the end of the condition.\",\n",
              " 'and then the statement to be executed conditionally is indented, so it is set off from the left   so that python knows that this statement is governed by this condition.',\n",
              " 'we make this simultaneous assignment of m taking the old value of n, and n taking the   value of m divided by n, only if m divided by n currently is not 0.',\n",
              " \"the second statement is executed conditionally, only if the condition evaluates to true and   indentation demarcates the body of the 'if'.\",\n",
              " 'the body refers to those statements which are governed by the condition that we have   just checked.',\n",
              " 'so, let us look at a small kind of illustration of this.',\n",
              " 'suppose, we have code which looks as follows; we have if condition then we have two indented   statements - statement 1 and statement 2, and then we have a third statement which is   not indented.',\n",
              " 'the indentation tells python that these two statements are conditionally executed depending   on the value of this condition.',\n",
              " 'however, statement 3 is not governed by this condition, because it is pushed out to the   same level as the if.',\n",
              " 'so, by governing by describing where your text lies, you can decide the beginning and   the end of what is governed by this condition.',\n",
              " 'in a conventional programming language, you would have some kind of punctuation typically   something like a brace to indicate the beginning and the end of the block, which is governed   by the condition.',\n",
              " 'one of the nice things about python which makes it easier to learn and to use and read   is the dispensation with many of these punctuations and syntactic things which make programming   languages difficult to understand.',\n",
              " 'so, when you are trying to learn a programming language, you would like to start programming   and not spend a lot of time understanding where to put colons, semicolons, open braces   and close braces and so on.',\n",
              " 'python tries to minimize this and that makes it an attractive language both to learn and   to write code in if you are doing certain kinds of things.',\n",
              " 'python will not have this and then we will see a much cleaner program as a result.',\n",
              " 'one thing we have emphasized before and, i will say it again is that this indentation   has to be uniform; in other words, it must be the same number of spaces.',\n",
              " 'the most dangerous thing you can do is to use a mixture of tabs and spaces, when you   press the tab on your keyboard it inserts some number of spaces which might look equal   to you when you see it on the screen, but python does not confuse tabs and spaces.',\n",
              " 'so, one tab is not going to be equal to three spaces or four spaces or whatever you see   on the screen; and the more worried thing is the python will give you some kind of error   message which is not very easy to understand.',\n",
              " 'so, it is quite useful to not get into the situation by remembering to always use exactly   some uniform strategy for example, two spaces to indent whenever you have such a nested   block.',\n",
              " 'quite often, we have two alternatives; one to be taken if the condition is true, and   the other to be taken if the condition is not true.',\n",
              " 'if it is likely to rain ensure the umbrella is in the bag, else ensure the umbrella is   not in the bag.',\n",
              " 'in this case for example, if the remainder is not 0 continue with new values for m and   n if the remainder is 0 then we have found the gcd namely the smaller with two values.',\n",
              " 'this is indicated using the such special word else which is like the english else again   with the colon and again, we have nesting to indicate what goes into the else and the   if and the else should be nested at the same level.',\n",
              " \"the else is optional, so we could have the 'if' without the else.\",\n",
              " \"technically speaking, the condition that we put into an 'if statement' must be an expression   that explicitly returns boolean value true or false.\",\n",
              " 'but like many other programming languages, python relaxes this a little bit and allows   different types of expressions which have different values like the types we have seen   so far like numbers and list to also be interpreted it as true or false.',\n",
              " 'in particular, any number, any expression, which returns a number 0, any numeric expression   of value 0 is treated as false.',\n",
              " 'similarly, any empty sequence such as the empty string or the empty list is also treated   as false.',\n",
              " 'and anything which is not in this case, so if i have a nonzero value as a number or if   i have a nonempty string a string with seven characters or a nonempty list a list with   three values then all of these would be interpreted as true if i just stick them into a condition.',\n",
              " 'so, this can simplify our expressions and our code.',\n",
              " 'instead of saying m percent n as we said before m percent n not equal to 0.',\n",
              " 'remember if it is not equal to 0, then it is true.',\n",
              " 'if this condition holds is the same as asking whether m percent n is a nonzero value, and   if it is nonzero value we want to replace m and n, so we can just write if m percent   n.',\n",
              " 'so this is a shortcut, now use it with care sometimes if you are used to it and if you   are familiar with what is going on, this can simplify the way you write things, but if   you are not familiar with what you are going on, you can make mistakes.',\n",
              " 'so, if you are in doubt, write the full comparison; if you are not in doubt or if it is very obvious,   then go with the shortcut.',\n",
              " 'here is a very common situation that occurs; sometimes we do not want to check between   one of two conditions, but one of many conditions.',\n",
              " 'so, supposing we have a value x, name x which can take a value 1, 2, 3 or 4 and depending   on whether it is 1, 2, 3 or 4, you want to do four different things with which we called   f1, f2, f3 and f4.',\n",
              " 'so, if we simulate this four way choice, using a if and else then we have to make some comparison   first.',\n",
              " 'supposing, we first check if x is equal to 1, then we invoke f1.',\n",
              " 'if x is not 1, then it is one of the others, so all of this goes into an else and everything   gets nested.',\n",
              " 'then we check in this case, if x is equal to 2 then we do f2 otherwise, we have 3 or   4, so now all of this is nested once again.',\n",
              " 'and finally, we have x is equal to 3 or not 3, so this is x is equal to 4 and then we   are done, but the main problem with this is that first of all this code is getting indented.',\n",
              " 'so, as you go into this nested if structure to simulate this multi-way branch that we   have essentially a four way branch x could take one of four values, where each of these   four values you want to do four different things.',\n",
              " 'if we simulate it by taking 4, 3, 2-way decisions, we check 1 or not 1, then we check 2 or not   2, and then we check 3 or not 3 then we have this ugly nesting and secondly, we have this   else followed immediately by an if.',\n",
              " 'python has a shortcut for this which is to combine the else and the if into a second   check elif.',\n",
              " 'so, this on the right is exactly the same as the left as for as python is concerned.',\n",
              " 'it checks the condition x is equal to 1, if the x is equal to 1 then it will invoke f1,   otherwise, it will invoke the rest.',\n",
              " 'now the rest, we have just collapse the else and if to directly check a new condition;   if this condition holds then this works; otherwise, it will check the rest and so on.',\n",
              " 'so, we can replace nested else if by elif in this way, and it makes the code more readable   because it tells us that we are actually doing a multi-way check.',\n",
              " 'and notice that the last word in elif is again an else.',\n",
              " 'so, if you have say seven different options, and you are only doing special things so 1,   2, 3, and 4, 5, 6, 7 are all the same then we can use else; we do not have explicitly   enumerate all the other option.',\n",
              " 'so, we have a number of explicit conditions that we check.',\n",
              " 'so, by the way this and notice the type or they should not be a ok.',\n",
              " 'so, we have a number of explicit conditions we check with if and a sequence or elifs,   and finally, we have an else, the else again is optional like it is with the normal if,   but the main thing is it avoids this long indentation sequence which makes the code   very difficult to read later on.',\n",
              " 'the first type of control flow which we have seen is conditional execution and the other   type is loops.',\n",
              " 'in a loop, we want to do something repeated number of times.',\n",
              " 'so, the most basic requirement is do something a fix number of times.',\n",
              " 'for instance, we have the statement for which is the keyword in python.',\n",
              " 'so, what python says is take a new name and let it run through a sequence of values, and   for each value in this sequence executes this once right.',\n",
              " 'so, this is in sequence, it is multiplying y by 1 then the result by 2 then the result   by 3 and so on.',\n",
              " 'the end of this will have y times 1 times, 2 times, 3 times 4, and we will have z plus   1 plus 1 plus 1 plus 1 - four times because each time we are adding 1 to z.',\n",
              " 'so, this should be outcome of this loop.',\n",
              " 'the main thing is exactly like if, we have indentation to mark the body of a loop.',\n",
              " 'the most common case for repeating things is to do something exactly n times.',\n",
              " 'so, instead of writing out a list 1 to n, what we saw is that we can generate something   equivalent to this list by using this built in function range.',\n",
              " 'the range 0, 1, we said it starts at 0 and it generates the sequence of the form 0, 1,   2 stopping at n minus 1.',\n",
              " 'this is similar to the similar positional convention in python which says that the positions   in a list go from 0 to the length of the list minus 1.',\n",
              " 'range also does not go from 0 to n, but 0 to n minus 1.',\n",
              " 'so, instead of writing for i in a list, we can write for i in a range, and this is from   the point of view of python the same thing, either we can let this new name range over   an explicit list or an implicit sequence given by the range function.',\n",
              " 'in general range i, j, like a slice i to j, starts at i and goes up to j minus 1.',\n",
              " 'we can also have range functions which count down and we can have range functions which   skip a value we can do every alternate value and so on.',\n",
              " 'we will see these variations on range a little later, but for now just note that we can either   have for statement which explicitly goes through a list of values.',\n",
              " 'so, we can give a list and ask for to go through each value in that list or we can generate   a sequence of n values by using the range function.',\n",
              " 'let us look at a simple example of this.',\n",
              " 'suppose, we want to find all the factors of a number n, all numbers that divide n without   a remainder.',\n",
              " 'as we recalled when we did gcd, all the divisors or factors of a number must lie between 1   and n; we cannot have any number bigger than n, which is a factor of n.',\n",
              " 'one simple way to check the list of factors is to try every number from 1 to n, and see   if it divides, so here is a very simple function for it.',\n",
              " 'so, we define a function called factors of n which is going to give back a list of all   the factors.',\n",
              " 'internally we use a name flist to the record this list.',\n",
              " 'the flist, the next list of factors is initially empty.',\n",
              " 'and now keeping in mind that all the factors lie between 1 and n, we generate in sequence   all the numbers from 1 to n, and remember this requires the upper bound of the range   to be n plus 1, because the range function stops one below the number which is the right   hand side.',\n",
              " 'so, this will generate a sequence 1, 2, 3 up to n.',\n",
              " 'and what we check is if there is no remainder when n is divided by i then we add i to this   list.',\n",
              " 'and remember that plus for a list and for a sequence is concatenation; it actually adds   a value to a list, and this allows us to return a new list.',\n",
              " 'the end of this, we have computed all the factors of n and return them as a list.',\n",
              " 'but as we said at the beginning of today’s lectures, sometimes we want to do something   some number of times without knowing the number of times in advance like stirring the sugar   in the cup.',\n",
              " 'we saw an example of this with the gcd.',\n",
              " 'so what we did was, we have another statement like for called while.',\n",
              " 'while executes something so long as a condition holds, so we execute this body so, long as   this condition evaluates to true, and then when we have finished executing this we come   back and check again whether this condition is true or not.',\n",
              " 'so the danger is that every time we come back the condition will be true and this thing   will never end in the for case we know in advance that it will execute exactly as many   times is length of the sequence that we started.',\n",
              " 'so, when we start a for loop we give a fix sequence that fix sequence has a fix length   and so we must terminate the loop in that many executions.',\n",
              " 'in a while we come back we check the condition again, but there is a every possibility that   the condition will never become true.',\n",
              " 'we have to ensure when we write a while loop that somehow this sequence of statements to   execute inside the while must eventually make this thing to be false, because if this thing   never become false, condition will never, the loop will never terminate.',\n",
              " 'the example we saw with gcd was the variation of what we wrote so far.',\n",
              " 'we first of course check and swap, so that the bigger number is first and now we check   whether the bigger number is divisible by the smaller.',\n",
              " 'so, so long as this is not the case, we exchange values for m and n.',\n",
              " 'we make m point to the smaller number n, and we make n point to the remainder, which will   be still smaller; and eventually this thing will become false, we will get the situation   where n divides m and the remainder is 0 and at that point we have found the gcd.',\n",
              " 'this is a simple example, remember that by our earlier convention we could also write   this as while m percent n make this thing.',\n",
              " 'so, we do not need this explicit not equal to 0, because the value m percent n if it   is not 0 is treated as true and the loop will go one more time, but now in some situations   it may or may not be so easy read this.',\n",
              " 'it might be useful to just say explicitly not equal to 0, just to illustrate to yourself   and to the person reading your code what is going on.',\n",
              " 'to summarize, a python interpreter normally executes code from top to bottom in sequence.',\n",
              " 'we can alter the control flow in three ways we have seen.',\n",
              " \"one is using the 'if statement', which conditionally executes and this extends to the elif which   allows us to do a multi-way branch.\",\n",
              " 'then there is for statement which allows us to repeat something a fixed number of times   that providing a list or a sequence of values from range.',\n",
              " 'and then we have a repetition which is based on a condition using a while statement where   we put a condition and then the body is executed each time the condition is checked again so   long this is true the body keeps repeating.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for val in eng_file_contents.keys():\n",
        "    if len(eng_file_contents[val]) != len(hin_file_contents[val]):\n",
        "        print(f\"For file {val} no of sentences are not matching\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9TVkoS2VNX1",
        "outputId": "2f74d96c-5e7c-4cd8-abf6-df09d3b65c67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For file algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt no of sentences are not matching\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(eng_file_contents[\"algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt\"]) ,len(hin_file_contents[\"algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7JVe5ldVNUx",
        "outputId": "0e1c78f9-e569-4c27-d5a5-87f7f9f83083"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(243, 244)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove the specified key from the dictionaries\n",
        "hin_file_contents.pop(\"algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt\", None)\n",
        "eng_file_contents.pop(\"algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt\", None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxSKYjrPVNSH",
        "outputId": "a857f62c-8fdb-4c81-f984-299e47e9f033"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['welcome to the first lecture on the course on programming data structures and algorithmin python.',\n",
              " \"let's start with the basic definition of what we mean by an algorithm and what programmingis.\",\n",
              " 'as most of you probably know, an algorithm is a description of how to systematicallyperform some task.',\n",
              " 'an algorithm consists of a sequence of steps which can we think ofas a recipe in order to achieve something.',\n",
              " 'so, the word recipe of course, comes fromcooking where we have list of ingredients and then a sequence of steps to prepare adish.',\n",
              " 'so, in the same way an algorithm is a way to prepare something or to achieve agiven task.',\n",
              " 'so, in the context of our thing, a recipe will is what we call a program.',\n",
              " 'andwe write down a program using a programming language.',\n",
              " 'so, the goal of programming languageis to be able to describe the sequence of steps that are required and to also describehow we might pursue different sequences of steps if different things happen in between.',\n",
              " 'the notion of a step is something that can be performed by whatever is executing thealgorithm.',\n",
              " 'now a program need not be executed by a machine although that will the typicalcontext of computer programming were we expect a computer to execute our steps.',\n",
              " 'a programcould also be executed by a person.',\n",
              " 'for instance, supposing the task at hand is to prepare ahall for a function.',\n",
              " 'so, this will consists of different steps such as a cleaning theroom, preparing the stage, making sure the decoration are up, arranging the chairs andso on.',\n",
              " 'this will be executed by a team of people.',\n",
              " 'now depending on the expertise andthe experience of this group of people, you can describe this algorithm at different levelsof detail.',\n",
              " 'for instance, an instruction such as arrangethe chair would makes sense if the peopleexploring  masculinities     involved know exactly what is expected.',\n",
              " 'onthe other hand, if this is a new group of people who have never done this before; youmight need to describe to step in more detail.',\n",
              " 'for instance, you might want to say that arrangethe chairs in the 8 rows and put 10 chairs in each row.',\n",
              " 'so, the notion of a step is subjective,it depends on what we expect of the person or the machine which is executing the algorithm.',\n",
              " 'and in terms of that capability, we describe the algorithm itself.',\n",
              " 'our focus in this course is going to be on computer algorithms and typically, these algorithmsmanipulate information.',\n",
              " 'the most basic kind of algorithm that all of us are familiar withfrom high school is an algorithm that computes numerical functions.',\n",
              " 'for instance, we couldhave an algorithm which takes two numbers x and y, and computes x to the power y.',\n",
              " 'so,we have seen any number of such functions in school.',\n",
              " 'for example, to compute square root of x, so what we do in school is we have complicatedway to compute square root of x or we might have x divided by y where we do long divisionand so on.',\n",
              " 'these are all algorithms, which compute values given one or more numbers theycompute the output of this function.',\n",
              " 'but all of us who have used computers know that many other things also fall within therealm of computation.',\n",
              " 'for instance, if we use a spreadsheet to arrange information andthen we want to sort of column.',\n",
              " 'so, this involves rearranging the items in the column in someorder either in ascending order or descending order.',\n",
              " 'so, reorganizing information is alsoa computational task and we need to know how to do this algorithmically.',\n",
              " 'we also see computationaround us in the day today’s life.',\n",
              " 'for instance, when we go to a travel booking site and wetry to book a flight from one city to another city it will offer to arrange the flightsin terms of the minimum time or the minimum cost.',\n",
              " 'so, these are optimization problems.',\n",
              " 'this involves also arranging information in a particular way and then computing some quantitythat we desire.',\n",
              " 'in this case, we want to know that a we canget from a to b, and b among all the ways we can get from a to b we want the optimumone.',\n",
              " 'and of course, there are many, many more things that we see day today, which are executedby computer programs.',\n",
              " 'we can play games.',\n",
              " 'for instance, we can solve sudoku or we can playchess against a program.',\n",
              " 'when we use the word processor to type a document or even whenwe use our cell phones to type sms messages, the computer suggests correction in our spelling.',\n",
              " 'we will look at some of these things in this course, but the point is to note that a programin our context is anything that looks at information and manipulates it to a given requirement.',\n",
              " 'so, it is not only a question of taking a number in and putting the number out.',\n",
              " 'it couldinvolve rearranging things.',\n",
              " 'it could involve computing more complicated things.',\n",
              " 'it couldinvolve organizing the information in a particular ways, so these computations become more tractableand that is what we call a data structure.',\n",
              " 'so to illustrate this let us look at the function which most of us have seen and try to understandthe algorithmically.',\n",
              " 'so, the property that i want to compute is the greatest common decidedivisor of two positive integers m and n.',\n",
              " 'so, as we know a divisor is a number thatdivides.',\n",
              " 'so k is a divisor of m; if i can divide m by k and get no reminder.',\n",
              " 'so, thegreatest common divisor of m and n must be something which is a common divisor.',\n",
              " 'so, commonmeans it must divide both and it must be the largest of these.',\n",
              " 'so, if the largest k suchthat k divides m and k also divides m.',\n",
              " 'for instance, if we look at the number 8 and12, then we can see that 4 is the factor of 8, 4 is the divisor of 8, 4 is also divisorof 12, another divisor of 12 is 6, but 6 is not a divisor of 8.',\n",
              " 'so, if we go through thedivisors of 8 and twelve it is easy to check that the largest number that divides both8 and 12 is 4.',\n",
              " 'so, gcd of 8 and 12 is 4.',\n",
              " 'what about 18 and 25.',\n",
              " '25 is 5 by 5.',\n",
              " 'so, it hasonly one divisor other than 1 and 25, which is 5.',\n",
              " 'and 5 is not a divisor of 18, but fortunately1 is a divisor of 18.',\n",
              " 'so, we can say that gcd of 18 and 25 is 1; there is no numberbigger than 1 that divides both 18 and 25.',\n",
              " 'since 1 divides every number, as we saw inthe case of 18 and 25, there will always be at least one common divisor among the twonumbers.',\n",
              " 'the gcd will always be well defined; it willnever be that we cannot find the common divisor and because all the common divisors will benumbers, we can arrange them from smallest to largest and pick the largest one as thegreatest common divisor.',\n",
              " 'so, given any pair of positive number m and n, we can definitelycompute the gcd of these two numbers.',\n",
              " 'so, how would one go about computing gcd of m, n? so, this is where we come to the algorithmicway, we want to describe the uniform way of systematically computing gcd of m n for anym or any n.',\n",
              " 'so, here is a very simple procedure.',\n",
              " 'it is not the most efficient; we will seebetter once as we go along.',\n",
              " 'but if we just look at the definition of gcd it says lookat all the factors of m, look at all the factor of n and find the largest one which is thefactor of both.',\n",
              " 'so, the naive way to do this would be first list out factors of m, thenlist out all the factor of second number n and then among these two lists report thelargest number that appears in both lists.',\n",
              " 'this is almost literally the definition ofgcd.',\n",
              " 'now question is does this constitute an algorithm.',\n",
              " 'well, at a high level of detail if we think of list out factors as a single step, whatwe want from an algorithm are two things.',\n",
              " 'one is that the description of what to domust be written down in a finite way.',\n",
              " 'in the sense that i should be able to write downthe instruction regardless of the value m and n in such a way it can read it and comprehendit once and for all.',\n",
              " 'here is very clear, we have exactly threesteps right.',\n",
              " 'so, we have three steps at constitute the algorithm so it certainly presented ina finite way.',\n",
              " 'the other requirement of an algorithm is that we must get the answer aftera finite number of steps.',\n",
              " 'of this finite number of steps may be different for different valuesof m and n, you can imagine that if you have a very small number for n there are not manyfactors they are the very large number for n you might have many factors.',\n",
              " 'so, the processof listing out the factors of m and n may take a long time; however, we want to be guaranteedthat this process will always end and then having done this we will always able to findthe largest number that appears in both lists.',\n",
              " 'to argue that this process is well defined all we need to realize is that the factorsof m must be between 1 and m.',\n",
              " \"in other words, we although there are infinitely many differentpossibility as factors we don't have to look at any number bigger than m, because it cannotgo into m evenly.\",\n",
              " 'so, all we need to do to compute the factors of m is to test everynumber is range one to m and if it divides m without a reminder, then we add it to listof factors.',\n",
              " 'so, we start with empty list of factors and we consider it on 1, 2, 3, 4 upto m and for each such number we check, whether if we divide m by this number we get a reminderof 0 we get a reminder of 0 we add it to the list.',\n",
              " 'let us look at the concrete example, let us try to compute the gcd of 14 and 63.',\n",
              " 'so, thefirst step in our algorithm says to compute the factors of 14.',\n",
              " 'so, by our observationabove the factors of 14 must lie between one and 14 nothing bigger than 14 can be a factor.',\n",
              " 'so, we start a listing our all the possible factors between one and 14 and testing them.',\n",
              " 'so, we know of course, that 1 will always divide; in this case 2 divides 14, because14 dived by 2 is 7 with no remainder.',\n",
              " 'now 3 does not divide, 4 does not divide, 5 doesnot divide, 6 does not divide; but 7 does, because if we divide 14 by 7 we get a remainderof 0.',\n",
              " 'then again 8 does not divide, nine does not divide and so on.',\n",
              " 'and finally, we find that the only other factor left is 14 itself.',\n",
              " 'so for every number m - 1and m will be factors and then there may be factors in between.',\n",
              " 'so, having done this we have identified that factors of 14 and these factors are preciselythe 1, 2, 7 and 14.',\n",
              " 'the next step in computing the gcd of 14 and 63 is to compute the factors of 63.',\n",
              " 'so, inthe same way we write down the all the numbers from one to 63 and we check which ones divide.',\n",
              " 'so, again we will find that 1 divides, here 2 does not divide; because 63 is not even,3 does divides, then we find a bunch of numbers here, which do not divide.',\n",
              " \"then we find that7 divides, because 7 9's are 63.\",\n",
              " 'then again 8 does not divides, but 9 does.',\n",
              " 'then againthere are large gap of numbers, which do not divide.',\n",
              " \"and then 21 does divide, because 213's are 63.\",\n",
              " 'and then finally, we find that the last factor that we have is 63.',\n",
              " 'so, if we go through this systematically from one to 63 crossing out each number which is nota factor we end up with the list 1, 3, 7, 9, 21 and 63.',\n",
              " 'having computed the factors of the two numbers 14 and 63 the next step in our algorithm saysthat we must find the largest factor that appears in both list.',\n",
              " 'so, how do we do this,how do we construct a list of common factors.',\n",
              " 'now there are more clever ways to do this,but here is a very simple way.',\n",
              " 'we just go through one of the lists say the list of factorsof 14 and for each item in the list we check it is a factor of 63.',\n",
              " 'so, we start with 1 and we say does 1 appear as a factor of 63.',\n",
              " 'it does so we add to thelist of common factors.',\n",
              " 'then we look at 2 then we ask does it appear; it does not appearso we skip it.',\n",
              " 'then we look at 3 and look at 7 rather and we find that 7 does appearso we add 7.',\n",
              " 'then finally, we look at 14 and find that 14 does not appears so we skip it.',\n",
              " 'in this way we have systematically gone through 1, 2, 7 and 14 and concluded that of theseonly 1 and 7 appear in both list.',\n",
              " 'and now having done this we have a list ofall the common factors we computed them from smallest to biggest, because we went to thefactors of 14 in ascending order.',\n",
              " 'so, this list will also be in ascending order.',\n",
              " 'so,returning the largest factors just returns the right most factor in this list namely7.',\n",
              " 'this is the output of our function.',\n",
              " 'we have computed the factors of 14, computedthe factors of 63, systematically checked for every factor of 14, whether it is alsoa factor of 63 and computed the list of common factors here and then from this list we haveextracted the largest one and this in fact, is our gcd.',\n",
              " 'this is an example of how thisalgorithm would execute.',\n",
              " 'if you have to write it down in little more detail, then we could say that we need tonotice that we need to remember these lists, right, and then come back to them.',\n",
              " 'so, weneed to compute the factors of 14 keep it side we need to write it down somewhere weneed to compute the factor of 63 write it down somewhere and then compare these twolists.',\n",
              " 'so, in other words we need to assign some names to store these.',\n",
              " 'let us call fmfor factors of m and fn factors of n as the names of these lists.',\n",
              " 'so, what we do is thatwe run through the numbers one to m.',\n",
              " 'and for each i, in this list 1 to m we check, whetheri divide m, whether m divided by i as reminder 0 and if so we add it to the list factorsof m or fm.',\n",
              " 'similarly, for each j from 1 to n we check, whether j divides n and if sowe add it to the list fn.',\n",
              " 'now we have two lists fm and fn which arethe factors of m and factors of n.',\n",
              " 'now we want to compute the list of common factors,which we will call cf.',\n",
              " 'so, what we do is for every f that is a factor of a first number,remember in our case was 14 where each f so we ran through 1, 2, 7 and 14 in our caseright.',\n",
              " 'so, for each f is list we add f to the list of the common factors if it alsoappears in the other list.',\n",
              " 'so, in the other list if you number is 1, 3, 7, 9, 21 and 63.',\n",
              " 'so, we compare f with this list and if we find it we add it to cf.',\n",
              " 'and having done this now we want to return the largest value of the list of common factors.',\n",
              " 'remember that one will always be a common factor.',\n",
              " 'so, the list cf will not be empty.',\n",
              " 'there will be at least one value, but since we add them in ascending order since the listfm and fn, where constructed from 1 to m and 1 to n the largest value will also be theright most value.',\n",
              " 'this gives us a slightly more detailed algorithm for gcd.',\n",
              " 'it is moreor less same as previous one except spells out little more details how to compute thelist of factors of m and how to compute the list of factors of n and how to compute thelargest of common factor between these two lists.',\n",
              " 'so, earlier we had three abstract statementsnow we are expanded out into 6, slightly more detailed statements.',\n",
              " 'this already gives us enough information to write our first python program.',\n",
              " 'of course,we will need to learn little more, before we know how to write it, but we can certainlyfigure out how to read it.',\n",
              " 'so, what this python programming is doing is exactly what we describedinformally in the previous step.',\n",
              " 'the first thing in the python program is a line whichdefines the function.',\n",
              " 'so, we are defining a function gcd of m comma n.',\n",
              " 'so, m and n arethe two arguments which could be any number like any function.',\n",
              " \"it's like when we readf of x y in mathematics it means x and y are arbitrator values and for every x and y dosomething depending on the values that we a call the function with.\",\n",
              " 'so, this says thatthis is a definition, so def for definition of a function gcd m, n.',\n",
              " 'now the first step is to compute the list of factors of m.',\n",
              " 'in python we write a listusing square brackets.',\n",
              " 'so, list is written as x y z and so on.',\n",
              " 'so, the empty list isjust an open bracket and a square close bracket.',\n",
              " 'so, we start off with an empty list of factors.',\n",
              " 'so, this equality means assign a value.',\n",
              " 'so, we assign fm the list of factors of m to bethe empty list.',\n",
              " 'now we need to test every value in the range 1 to n.',\n",
              " 'now python has a built in function called range, but because of we shall see, becauseof peculiarity of python this returns not the range you except, but one less.',\n",
              " 'so, ifi say give the numbers in the range 1 to n plus 1, it gives me in the range one to m,one up to the upper limit, but not including the upper limit.',\n",
              " 'so, this will say that itakes the values one two three up to m.',\n",
              " 'for each of these values of i, we check whetherthis is true.',\n",
              " 'now percentage is the remainder operation.',\n",
              " 'it checks whether remainder of m divided by i is 0.',\n",
              " 'if the remainder of m divided by iis 0 then we will append i to the list fn, we will add it to the right append is theenglish word which just means add to the end of the list.',\n",
              " 'so, we append i to n.',\n",
              " 'so, inthis step, we have computed fm.',\n",
              " 'this is exactly what we wrote informally in the previous examplewe just said that for each i from 1 to m add i to fm if i divides m and now we have doneit in python syntax.',\n",
              " 'so, we have defined an empty list of factors and for each numberin that range we have checked it is a divisor and then add it.',\n",
              " 'and now here we do the exactly the same thing for n.',\n",
              " 'so, we start with the empty list offactors of n for every j in for this range if it divides we append it.',\n",
              " 'now, at this pointwe have two list fm and fn.',\n",
              " 'now, we want to compute the list of common factors.',\n",
              " 'so, weuse cf to denote the list of common factors.',\n",
              " 'initially there are no common factors.',\n",
              " 'now,for every factor in the first list if the factor appears in the second list then weappend it to cf.',\n",
              " 'so, the same function append is being usethroughout.',\n",
              " 'just take a list and add a value.',\n",
              " 'which value? we add the value that we arelooking at now provided it satisfies the conditions.',\n",
              " 'so, earlier we were adding provided the divisorwas 0 uh the remainder was 0, now we are adding it provided it appears in both list.',\n",
              " 'for everyf in the first list if it appears in the second list add it.',\n",
              " 'after this we have computed fm, cf.',\n",
              " 'and now we want the right most element.',\n",
              " 'so, this isjust some python syntax if you see which says that instead of, if we start counting fromthe left then the number the positions in the list are number 0, 1, 2, 3, and 4.',\n",
              " 'butpython has a shortcut which says that you want to count from the right then we countthe numbers as minus 1, minus 2 and so on.',\n",
              " \"so, it says return the minus 1'th elementof cf which in python jargon means return the right most element.\",\n",
              " 'so, this is the rightmost element.',\n",
              " 'at this point it is enough to understand thatwe can actually try and decode this code this program even though we may not understandexactly why we are using colon in some places and why we are pushing something.',\n",
              " 'see noticethat are other syntactic things here, so there are for example, you have these punctuationmarks, which are a bit odd like these colons.',\n",
              " 'then you have the fact that this line is indentedwith respect to this line, this line is indented to this line.',\n",
              " 'these are all features that make python programs a little easier to read and write then programsin other languages.',\n",
              " 'so, we will come to these when we learn python syntax more formally.',\n",
              " 'but at this point you should be able to convince yourself that this set of python steps isa very faithful rendering of the informal algorithm that we wrote in the previous slide.',\n",
              " 'let us note some points that we can already deduce from this particular example.',\n",
              " 'so, thefirst important point is that we need a way to keep track of intermediate values.',\n",
              " 'so,we have two names to begin with the names of our arguments m and n.',\n",
              " 'then we use thesethree names to compute this list of factors and common factors and we use other nameslike i, j and f.',\n",
              " 'in order to run through these.',\n",
              " 'we need i to run from 1 to n.',\n",
              " 'we need j torun from 1 to n.',\n",
              " 'of course, we could reuse i.',\n",
              " 'but it is okay.',\n",
              " 'we use f to run throughall the factors in cf.',\n",
              " 'so, these are all ways of keeping track of intermediate values.',\n",
              " 'thesecond point to note is that a value can be a single item.',\n",
              " 'for example, m n are numbers, similarly i, j and f at each step are numbers.',\n",
              " 'so, thesewill be single values or they could be collections.',\n",
              " 'so, there are lists.',\n",
              " 'so fm is a list, fn isa list.',\n",
              " 'so, it is a single name denoting a collection of values in this case a list asequence has a first position and next position and a last position.',\n",
              " 'these are list of numbers.',\n",
              " 'one can imagine the other collections and we will see them as we go along.',\n",
              " 'so, collectionsare important, because it would be very difficult to write a program if we had to keep producinga name for every factor of m separately.',\n",
              " 'we need a name collectively for all the factorsof m regardless of how big m is.',\n",
              " 'these names can denote can be denote single values orcollections of values.',\n",
              " 'and a collection of values with the particular structure is preciselywhat we call data structure.',\n",
              " 'so, these are more generally called data structures.',\n",
              " 'so,in this case the data structure we have is a list.',\n",
              " 'what can we do with these names and values well one thing is we can assigned a valueto a name.',\n",
              " 'so, for instance when we write fn is equal to the empty list we are explicitlysetting the value of fn to be the empty list.',\n",
              " 'this tells two things this says the valueis empyt list, so it is also tells python the fn denotes the lists these are the twosteps going on here as we see.',\n",
              " 'and the other part is that when we write somethinglike for each f in the list cf, which is implicitly saying that take every value in cf and assignit one by one to the values f to the name f.',\n",
              " 'right though they do not have this equalitysign explicitly implicitly this is assigning the new values for f as we step the list cfright.',\n",
              " 'so, the main thing that we do in a python program is to assign values to names.',\n",
              " 'and having assigned a value we can then modify the value.',\n",
              " 'for instance every time we finda new factor of n we do not want to through any old factor we want to take the existinglist fm and we want to add it.',\n",
              " 'so, this function append for instance modifies the value ofthe name fn to a new name which takes the old name and sticks an i at the end of it.',\n",
              " 'more generally you could have a number and we could want a replaces by two times a number.',\n",
              " 'so, we might have something like i is equal to two times i.',\n",
              " 'so, star stands for multiplicationthis does not mean that i is equals to two times i arithmetically because; obviously,unless i is 0 i cannot be equal to two times itself.',\n",
              " 'what is means is that take the currentvalue of i, multiply it by two and assign it to i.',\n",
              " 'so, we will see this as we go along,but assignment can either assign a completely new value or you could update the value usingthe old value.',\n",
              " 'so, here we taking the old value of the function of the list fn and weare appending a value it would getting a new value of fn.',\n",
              " 'the other part that we are need to note is how we execute steps.',\n",
              " \"so, we said at the beginningof today's lecture a program is a sequence of steps.\",\n",
              " 'but we do not just execute the sequenceof steps from beginning to end blindly.',\n",
              " 'sometimes we have to do the same thing again and again.',\n",
              " 'for instance we have to check for every possible factor from 1 to m if it divides m and thenput it in the list.',\n",
              " 'so, some steps are repeated we do something, for examples here for eachitem in a list.',\n",
              " 'and some steps are executed only if the valuethat we are looking at meets particular conditions.',\n",
              " 'when we say something like if m percent iis 0, if the remainder of m divided by a is 0 then append.',\n",
              " 'so, the step append i to fmthe factors of m this happens only if i matches the condition that it is a factor of m.',\n",
              " 'so,we have repeated steps where same thing done again and again.',\n",
              " 'and they have conditionalssteps something which is done only if a particular condition holds.',\n",
              " 'so, we will stop here.',\n",
              " 'these examples should show you that programs are not very differentfrom what we know intuitively, it is only a question of writing them down correctly,and making sure that we keep track of all the intermediate values and steps that weneed as we long, so that we do not lose things.',\n",
              " 'we will look at this example in more detailas we go long, and try to find other ways of writing it, and examine other features,but essentially this is a good way of illustrating programming.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_hindi = []\n",
        "for file_name in hin_file_contents.keys():\n",
        "    for sentence in hin_file_contents[file_name]:\n",
        "        sentences_hindi.append(sentence)\n",
        "\n",
        "sentences_english = []\n",
        "for file_name in eng_file_contents.keys():\n",
        "    for sentence in eng_file_contents[file_name]:\n",
        "        sentences_english.append(sentence)"
      ],
      "metadata": {
        "id": "9Kr9xTrAVVsB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences_english) == len(sentences_hindi), len(sentences_hindi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3yNkXjGVVpa",
        "outputId": "19751490-faaf-431e-c9b2-48cbcdb2457a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 5771)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_english[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CL9EEFIYVVmj",
        "outputId": "20b1837d-f7cd-4981-9d9f-bc35c995665c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in the lecture, we saw that in object oriented programming we define a data type througha template called a class, which defines the internal data implementation, and the functionsthat we use to manipulate the data type.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IIT BOMBAY DATASET"
      ],
      "metadata": {
        "id": "XX_FcpUeVcyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX7Ta_V4VfOW",
        "outputId": "16f60c01-93f3-42ce-9620-64fc51cf0742"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "\n",
        "print(dataset.keys())\n",
        "print(dataset['train'])\n",
        "print(len(dataset['train'][\"translation\"][0]))\n",
        "print(dataset['train'][\"translation\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519,
          "referenced_widgets": [
            "cbfccfe815614271a0354c589988316b",
            "5a5f3a82550d4d79b80416b36b17e97f",
            "d8d1be74722248a09b8bdf447007025e",
            "43c3da295bdd4fc7a0c19e940bb7d4f9",
            "35d8da9d5fb643e0ab279cd2f89db3f2",
            "7f67e0c6e58b4267bc3a7c9c7072dc20",
            "5acbd0303dd74029909815286074490f",
            "cef1cd78453144a4922d03ac495c90f5",
            "5ea9408932b54245974015634bf4d98b",
            "b867d1ffbd964dcdb95a3b4f0cde2600",
            "31a04ca7ca904e5c90d3fcf95a1e6578",
            "3ba0a9dd73f24a6994814470f047c545",
            "18ea26b1d471491090064cbf10868689",
            "1d709171e19747f586962aeeb51113ae",
            "c7320d5f4b66428b9c7301d07929f812",
            "5ea375fe22a4448ab46e1d9b54abf25b",
            "8162a4b77f7540cbad4cbaa599018957",
            "0c425dc43acf4f44a4890ff93b235718",
            "59272b8e7b4b4938b589a980c7ba0f51",
            "dcf95a811fb44d82985bd533b6043156",
            "d6874123f66c46d0b116410a25332eb0",
            "c041d6b3ed0a4e39befec67b87c36b3c",
            "4845dc77161f4e84af2e05a219311f16",
            "feebb83b6f7847d5ac3e52046936577d",
            "a5b452eb4a414023802a8dd28509b2f9",
            "5aa25f5eee62409eb1a02b84770d142a",
            "d28cdee519134278ac5b4521452f4e99",
            "611c43b7b2ae4ea3ae642cd65bb29c4c",
            "63ac8d066ba74c6f8ba6dc025e221b9e",
            "4b0bc7b3f751446ba0497deaf7c99578",
            "57078adc703441ef8425beeed6e5114e",
            "865706fe5cae4b5ebdf22e3182030d0f",
            "6123694393854a15889420ea90f4ce38",
            "293f8c58c60442968ca4e10aea19cdc0",
            "5b5814c554844f7598d8f8101b38190f",
            "e78a0a85086f42be8ff2fb7f0140614f",
            "5b8c00cd03184993b1d071a61dce1dd5",
            "fa27b76cb41349fe97f11fbd3fd05bf5",
            "455b61902584403b8186b55f99230529",
            "879d74172cb045eb9667a35820c427e9",
            "94e78cdc3b214fb7804b48878caeb04a",
            "8d3a66c81d5447919b75234553e9e61f",
            "43d2fcbbe7454d70a5b4648548902e7f",
            "66b6ac687dab483a96867656565f61eb",
            "361df7593fc447eb87c2ab4af6ec5975",
            "84ef6e5cea5347f4bc4e8357176f8581",
            "7805c898692b4872a9f8a88ffa6b5c64",
            "d3738b34fd1c4f45ad7d03feb1431072",
            "9f1d9ea605b640bbb701bb3106e877e1",
            "7d075f9d669b498ab5e4c700dfd62397",
            "485ad9006d8c4f69b09d59a47c7322d3",
            "7b1809fe39fa4b6bbe58b6f6181549b1",
            "9428d095398c48b583c1fe9860881ce4",
            "cb468b2793664e57b6e5520f1e68e390",
            "dbf3cc7fb90642c88851c6f114f23a9e",
            "0edc1e3af741445dbd7c7375b66a3358",
            "67c4c81f35744801b904f68874034a39",
            "cd4e5ca2a7eb4de985db85f1bc007f75",
            "8ef6780b995d4821a9617820b840cc74",
            "bc6be9d30fd740ebbd8f7bbd2b384d5f",
            "989bc6a41a4c4938ae647e721b299365",
            "cb8b25f64b3c442faca5f8d758797528",
            "3687cdea9a7247dea956d9ab6d103282",
            "7798b592f25f4b4b8f3e008a94838bde",
            "83a3d05ed09543719737b1778feff09e",
            "7b38bead326042a6a6dd200001b3eb50",
            "ef4bfd622fbb4dffb1933410d477a402",
            "6460f75a98c843bf94f5a9d233b2a27b",
            "b599371194134604b11b35b456084538",
            "95f45918a6744d1ba7604eaf9d071eef",
            "fbce4086ade248d6a2af896cb91aaf4c",
            "bf4e42d6f3a941aab7c0fbe151e89989",
            "a51a7c7dc42c4a6cb6043fcf6714ae69",
            "7711c1113a804f64afee5ce9f5af72ab",
            "8746dc7b112a4bc3bdef99fc8ae8dcf2",
            "c12b3373c30a4d43acc668a21e3f3cad",
            "41adb8a9a9cb40ee9d5e96d6847d40ed",
            "7d1a97338977487fa382fee83afb4500",
            "1ea3f28827bb4c77912089d1c8b83c65",
            "0364d83e00854ea988da44153729bd7d",
            "7b25913d86f74c39bb15189b4bbeee62",
            "ad8caa4a0adf433aa6b940d4b214cdbb",
            "beaa52119428406ebcf2fb03350bea0b",
            "5611c24a255246a186495ea6803dde0f",
            "e2875cdd66534990a28fb502356481b7",
            "8ca832b97a644c3bbd9775704d74e002",
            "e8310ba66509492b90605acb2e4f599c",
            "02dbe1d661c54b6ca0e77660ce861178"
          ]
        },
        "id": "ggMBevo0VfLk",
        "outputId": "d9505d22-3b6f-4446-c638-5bb076bc047f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/3.14k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbfccfe815614271a0354c589988316b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/953 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ba0a9dd73f24a6994814470f047c545"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4845dc77161f4e84af2e05a219311f16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/85.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "293f8c58c60442968ca4e10aea19cdc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "361df7593fc447eb87c2ab4af6ec5975"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0edc1e3af741445dbd7c7375b66a3358"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef4bfd622fbb4dffb1933410d477a402"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d1a97338977487fa382fee83afb4500"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['train', 'validation', 'test'])\n",
            "Dataset({\n",
            "    features: ['translation'],\n",
            "    num_rows: 1659083\n",
            "})\n",
            "2\n",
            "{'en': 'Give your application an accessibility workout', 'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for translation_pair in dataset[\"train\"][\"translation\"][:500000]:\n",
        "  sentences_hindi.append(translation_pair[\"hi\"])\n",
        "  sentences_english.append(translation_pair[\"en\"])"
      ],
      "metadata": {
        "id": "2ADjysmXVfI2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_text = sentences_hindi\n",
        "eng_text = sentences_english"
      ],
      "metadata": {
        "id": "2ry_oOH3VsR8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Loading hindi text\n",
        "# with open(\"C:\\\\Users\\\\Aditya Singh\\\\Desktop\\\\Deep Learning\\\\7. Language Modelling\\\\English to Hindi\\\\pruned_train.hi\", \"r\", encoding='utf-8') as hindi_inp:\n",
        "#     _text = hindi_inp.read()\n",
        "# hindi_text = _text.split('\\n')\n",
        "# # len(hindi_text)  -- 788099\n",
        "\n",
        "\n",
        "# # Loading english text\n",
        "# with open(\"C:\\\\Users\\\\Aditya Singh\\\\Desktop\\\\Deep Learning\\\\7. Language Modelling\\\\English to Hindi\\\\pruned_train.en\", \"r\", encoding='utf-8') as eng_inp:\n",
        "#     _text = eng_inp.read()\n",
        "# eng_text = _text.split('\\n')\n",
        "# # len(eng_text) -- 788099\n",
        "\n"
      ],
      "metadata": {
        "id": "9Vd_QsMLTmnJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import copy\n",
        "\n",
        "\n",
        "# Importing contractions\n",
        "with open(\"/content/drive/MyDrive/ssmt/contractions.txt\", \"r\") as inp_cont:\n",
        "    contractions_list = inp_cont.read()\n",
        "contractions_list = [re.sub('[\"]', '', x).split(\":\") for x in re.sub(r\"\\s+\", \" \", re.sub(r\"(.*{)|(}.*)\", '', contractions_list)).split(',')]\n",
        "contractions_dict = dict((k.lower().strip(), re.sub('/.*', '', v).lower().strip()) for k, v in contractions_list)"
      ],
      "metadata": {
        "id": "T3NrPQwXWL5j"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6dxbxtPWVW4",
        "outputId": "086fee72-5f3d-47b7-e7e8-88f6f03010e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"ain't\": 'am not',\n",
              " \"aren't\": 'are not',\n",
              " \"can't\": 'cannot',\n",
              " \"can't've\": 'cannot have',\n",
              " \"'cause\": 'because',\n",
              " \"could've\": 'could have',\n",
              " \"couldn't\": 'could not',\n",
              " \"couldn't've\": 'could not have',\n",
              " \"didn't\": 'did not',\n",
              " \"doesn't\": 'does not',\n",
              " \"don't\": 'do not',\n",
              " \"hadn't\": 'had not',\n",
              " \"hadn't've\": 'had not have',\n",
              " \"hasn't\": 'has not',\n",
              " \"haven't\": 'have not',\n",
              " \"he'd\": 'he had',\n",
              " \"he'd've\": 'he would have',\n",
              " \"he'll\": 'he shall',\n",
              " \"he'll've\": 'he shall have',\n",
              " \"he's\": 'he has',\n",
              " \"how'd\": 'how did',\n",
              " \"how'd'y\": 'how do you',\n",
              " \"how'll\": 'how will',\n",
              " \"how's\": 'how has',\n",
              " \"i'd\": 'i had',\n",
              " \"i'd've\": 'i would have',\n",
              " \"i'll\": 'i shall',\n",
              " \"i'll've\": 'i shall have',\n",
              " \"i'm\": 'i am',\n",
              " \"i've\": 'i have',\n",
              " \"isn't\": 'is not',\n",
              " \"it'd\": 'it had',\n",
              " \"it'd've\": 'it would have',\n",
              " \"it'll\": 'it shall',\n",
              " \"it'll've\": 'it shall have',\n",
              " \"it's\": 'it has',\n",
              " \"let's\": 'let us',\n",
              " \"ma'am\": 'madam',\n",
              " \"mayn't\": 'may not',\n",
              " \"might've\": 'might have',\n",
              " \"mightn't\": 'might not',\n",
              " \"mightn't've\": 'might not have',\n",
              " \"must've\": 'must have',\n",
              " \"mustn't\": 'must not',\n",
              " \"mustn't've\": 'must not have',\n",
              " \"needn't\": 'need not',\n",
              " \"needn't've\": 'need not have',\n",
              " \"o'clock\": 'of the clock',\n",
              " \"oughtn't\": 'ought not',\n",
              " \"oughtn't've\": 'ought not have',\n",
              " \"shan't\": 'shall not',\n",
              " \"sha'n't\": 'shall not',\n",
              " \"shan't've\": 'shall not have',\n",
              " \"she'd\": 'she had',\n",
              " \"she'd've\": 'she would have',\n",
              " \"she'll\": 'she shall',\n",
              " \"she'll've\": 'she shall have',\n",
              " \"she's\": 'she has',\n",
              " \"should've\": 'should have',\n",
              " \"shouldn't\": 'should not',\n",
              " \"shouldn't've\": 'should not have',\n",
              " \"so've\": 'so have',\n",
              " \"so's\": 'so as',\n",
              " \"that'd\": 'that would',\n",
              " \"that'd've\": 'that would have',\n",
              " \"that's\": 'that has',\n",
              " \"there'd\": 'there had',\n",
              " \"there'd've\": 'there would have',\n",
              " \"there's\": 'there has',\n",
              " \"they'd\": 'they had',\n",
              " \"they'd've\": 'they would have',\n",
              " \"they'll\": 'they shall',\n",
              " \"they'll've\": 'they shall have',\n",
              " \"they're\": 'they are',\n",
              " \"they've\": 'they have',\n",
              " \"to've\": 'to have',\n",
              " \"wasn't\": 'was not',\n",
              " \"we'd\": 'we had',\n",
              " \"we'd've\": 'we would have',\n",
              " \"we'll\": 'we will',\n",
              " \"we'll've\": 'we will have',\n",
              " \"we're\": 'we are',\n",
              " \"we've\": 'we have',\n",
              " \"weren't\": 'were not',\n",
              " \"what'll\": 'what shall',\n",
              " \"what'll've\": 'what shall have',\n",
              " \"what're\": 'what are',\n",
              " \"what's\": 'what has',\n",
              " \"what've\": 'what have',\n",
              " \"when's\": 'when has',\n",
              " \"when've\": 'when have',\n",
              " \"where'd\": 'where did',\n",
              " \"where's\": 'where has',\n",
              " \"where've\": 'where have',\n",
              " \"who'll\": 'who shall',\n",
              " \"who'll've\": 'who shall have',\n",
              " \"who's\": 'who has',\n",
              " \"who've\": 'who have',\n",
              " \"why's\": 'why has',\n",
              " \"why've\": 'why have',\n",
              " \"will've\": 'will have',\n",
              " \"won't\": 'will not',\n",
              " \"won't've\": 'will not have',\n",
              " \"would've\": 'would have',\n",
              " \"wouldn't\": 'would not',\n",
              " \"wouldn't've\": 'would not have',\n",
              " \"y'all\": 'you all',\n",
              " \"y'all'd\": 'you all would',\n",
              " \"y'all'd've\": 'you all would have',\n",
              " \"y'all're\": 'you all are',\n",
              " \"y'all've\": 'you all have',\n",
              " \"you'd\": 'you had',\n",
              " \"you'd've\": 'you would have',\n",
              " \"you'll\": 'you shall',\n",
              " \"you'll've\": 'you shall have',\n",
              " \"you're\": 'you are',\n",
              " \"you've\": 'you have',\n",
              " \"ain ' t\": 'am not',\n",
              " \"aren ' t\": 'are not',\n",
              " \"can ' t\": 'cannot',\n",
              " \"can ' t ' ve\": 'cannot have',\n",
              " \"' cause\": 'because',\n",
              " \"could ' ve\": 'could have',\n",
              " \"couldn ' t\": 'could not',\n",
              " \"couldn ' t ' ve\": 'could not have',\n",
              " \"didn ' t\": 'did not',\n",
              " \"doesn ' t\": 'does not',\n",
              " \"don ' t\": 'do not',\n",
              " \"hadn ' t\": 'had not',\n",
              " \"hadn ' t ' ve\": 'had not have',\n",
              " \"hasn ' t\": 'has not',\n",
              " \"haven ' t\": 'have not',\n",
              " \"he ' d\": 'he had',\n",
              " \"he ' d ' ve\": 'he would have',\n",
              " \"he ' ll\": 'he shall',\n",
              " \"he ' ll ' ve\": 'he shall have',\n",
              " \"he ' s\": 'he has',\n",
              " \"how ' d\": 'how did',\n",
              " \"how ' d ' y\": 'how do you',\n",
              " \"how ' ll\": 'how will',\n",
              " \"how ' s\": 'how has',\n",
              " \"i ' d\": 'i had',\n",
              " \"i ' d ' ve\": 'i would have',\n",
              " \"i ' ll\": 'i shall',\n",
              " \"i ' ll ' ve\": 'i shall have',\n",
              " \"i ' m\": 'i am',\n",
              " \"i ' ve\": 'i have',\n",
              " \"isn ' t\": 'is not',\n",
              " \"it ' d\": 'it had',\n",
              " \"it ' d ' ve\": 'it would have',\n",
              " \"it ' ll\": 'it shall',\n",
              " \"it ' ll ' ve\": 'it shall have',\n",
              " \"it ' s\": 'it has',\n",
              " \"let ' s\": 'let us',\n",
              " \"ma ' am\": 'madam',\n",
              " \"mayn ' t\": 'may not',\n",
              " \"might ' ve\": 'might have',\n",
              " \"mightn ' t\": 'might not',\n",
              " \"mightn ' t ' ve\": 'might not have',\n",
              " \"must ' ve\": 'must have',\n",
              " \"mustn ' t\": 'must not',\n",
              " \"mustn ' t ' ve\": 'must not have',\n",
              " \"needn ' t\": 'need not',\n",
              " \"needn ' t ' ve\": 'need not have',\n",
              " \"o ' clock\": 'of the clock',\n",
              " \"oughtn ' t\": 'ought not',\n",
              " \"oughtn ' t ' ve\": 'ought not have',\n",
              " \"shan ' t\": 'shall not',\n",
              " \"sha ' n ' t\": 'shall not',\n",
              " \"shan ' t ' ve\": 'shall not have',\n",
              " \"she ' d\": 'she had',\n",
              " \"she ' d ' ve\": 'she would have',\n",
              " \"she ' ll\": 'she shall',\n",
              " \"she ' ll ' ve\": 'she shall have',\n",
              " \"she ' s\": 'she has',\n",
              " \"should ' ve\": 'should have',\n",
              " \"shouldn ' t\": 'should not',\n",
              " \"shouldn ' t ' ve\": 'should not have',\n",
              " \"so ' ve\": 'so have',\n",
              " \"so ' s\": 'so as',\n",
              " \"that ' d\": 'that would',\n",
              " \"that ' d ' ve\": 'that would have',\n",
              " \"that ' s\": 'that has',\n",
              " \"there ' d\": 'there had',\n",
              " \"there ' d ' ve\": 'there would have',\n",
              " \"there ' s\": 'there has',\n",
              " \"they ' d\": 'they had',\n",
              " \"they ' d ' ve\": 'they would have',\n",
              " \"they ' ll\": 'they shall',\n",
              " \"they ' ll ' ve\": 'they shall have',\n",
              " \"they ' re\": 'they are',\n",
              " \"they ' ve\": 'they have',\n",
              " \"to ' ve\": 'to have',\n",
              " \"wasn ' t\": 'was not',\n",
              " \"we ' d\": 'we had',\n",
              " \"we ' d ' ve\": 'we would have',\n",
              " \"we ' ll\": 'we will',\n",
              " \"we ' ll ' ve\": 'we will have',\n",
              " \"we ' re\": 'we are',\n",
              " \"we ' ve\": 'we have',\n",
              " \"weren ' t\": 'were not',\n",
              " \"what ' ll\": 'what shall',\n",
              " \"what ' ll ' ve\": 'what shall have',\n",
              " \"what ' re\": 'what are',\n",
              " \"what ' s\": 'what has',\n",
              " \"what ' ve\": 'what have',\n",
              " \"when ' s\": 'when has',\n",
              " \"when ' ve\": 'when have',\n",
              " \"where ' d\": 'where did',\n",
              " \"where ' s\": 'where has',\n",
              " \"where ' ve\": 'where have',\n",
              " \"who ' ll\": 'who shall',\n",
              " \"who ' ll ' ve\": 'who shall have',\n",
              " \"who ' s\": 'who has',\n",
              " \"who ' ve\": 'who have',\n",
              " \"why ' s\": 'why has',\n",
              " \"why ' ve\": 'why have',\n",
              " \"will ' ve\": 'will have',\n",
              " \"won ' t\": 'will not',\n",
              " \"won ' t ' ve\": 'will not have',\n",
              " \"would ' ve\": 'would have',\n",
              " \"wouldn ' t\": 'would not',\n",
              " \"wouldn ' t ' ve\": 'would not have',\n",
              " \"y ' all\": 'you all',\n",
              " \"y ' all ' d\": 'you all would',\n",
              " \"y ' all ' d ' ve\": 'you all would have',\n",
              " \"y ' all ' re\": 'you all are',\n",
              " \"y ' all ' ve\": 'you all have',\n",
              " \"you ' d\": 'you had',\n",
              " \"you ' d ' ve\": 'you would have',\n",
              " \"you ' ll\": 'you shall',\n",
              " \"you ' ll ' ve\": 'you shall have',\n",
              " \"you ' re\": 'you are',\n",
              " \"you ' ve\": 'you have'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_sc(_line, lang=\"en\"):\n",
        "    # _line = copy.deepcopy(_line)\n",
        "    if lang == \"hi\":\n",
        "        _line = re.sub(r'[+\\-*/#@%>=;~{}×–`’\"()_]', \"\", _line)\n",
        "        _line = re.sub(r\"(?:(\\[)|(\\])|(‘‘)|(’’))\", '', _line)\n",
        "    elif lang == \"en\":\n",
        "        _line = re.sub(r'[+\\-*/#@%>=;~{}×–`’\"()_|:]', \"\", _line)\n",
        "        _line = re.sub(r\"(?:(\\[)|(\\])|(‘‘)|(’’))\", '', _line)\n",
        "    return _line\n",
        "\n",
        "\n",
        "def clean_text(_text, lang=\"en\"):\n",
        "    # _text = copy.deepcopy(_text)\n",
        "    if lang == \"en\":\n",
        "        _text = remove_sc(_line=_text, lang=lang)\n",
        "        for cn in contractions_dict:\n",
        "            _text = re.sub(cn, contractions_dict[cn], _text)\n",
        "    elif lang == \"hi\":\n",
        "        _text = remove_sc(_line=_text, lang=lang)\n",
        "    return _text"
      ],
      "metadata": {
        "id": "zNUeu2m0UNNI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Removing Hindi sentences having english letter in it\n",
        "ids_to_remove = {}\n",
        "for _id, _t in tqdm(enumerate(hindi_text)):\n",
        "    if len(re.findall(r'[a-zA-Z]', _t)) > 0:\n",
        "        ids_to_remove[_id] = _t\n",
        "        # ids_to_remove.append(_id)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "ids_to_keep = [i for i in range(len(hindi_text)) if i not in ids_to_remove.keys()]\n",
        "filtered_eng_text = []\n",
        "filtered_hindi_text = []\n",
        "for _id in tqdm(ids_to_keep):\n",
        "    filtered_eng_text.append(eng_text[_id].lower())\n",
        "    filtered_hindi_text.append(hindi_text[_id])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZzZmKFDToep",
        "outputId": "61540e9a-8eab-4319-a8b0-12d587db0e0e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "505771it [00:04, 106644.14it/s]\n",
            "100%|██████████| 411701/411701 [00:00<00:00, 536972.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Treating english sentences\n",
        "clean_eng_text = []\n",
        "for sent in tqdm(filtered_eng_text):\n",
        "    clean_eng_text.append(clean_text(_text=copy.deepcopy(sent), lang=\"en\"))\n",
        "\n",
        "\n",
        "# Treating hindi sentences\n",
        "clean_hindi_text = []\n",
        "for sent in tqdm(filtered_hindi_text):\n",
        "    clean_hindi_text.append(clean_text(_text=copy.deepcopy(sent), lang=\"hi\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3yEJ4DcWjmq",
        "outputId": "b8d740e7-9fa8-4acc-c086-95da72b4c49e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411701/411701 [01:51<00:00, 3703.74it/s]\n",
            "100%|██████████| 411701/411701 [00:04<00:00, 95700.07it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filtered Data\n",
        "clean_data = pd.DataFrame({\"eng_text\": clean_eng_text, \"hindi_text\": clean_hindi_text})\n",
        "\n",
        "\n",
        "# Filtering data based on sentence length\n",
        "clean_data[\"eng_len\"] = clean_data.eng_text.str.count(\" \")\n",
        "clean_data[\"hindi_len\"] = clean_data.hindi_text.str.count(\" \")\n",
        "small_len_data = clean_data.query('eng_len < 50 & hindi_len < 50')"
      ],
      "metadata": {
        "id": "4jiezh1iWk5m"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Val split\n",
        "# Full set\n",
        "train_set, val_set = train_test_split(small_len_data.loc[:, [\"eng_text\", \"hindi_text\"]], test_size=0.1)\n",
        "train_set.to_csv(\"/content/drive/MyDrive/ssmt/train.csv\", index=False)\n",
        "val_set.to_csv(\"/content/drive/MyDrive/ssmt/val.csv\", index=False)\n",
        "\n",
        "# Small set\n",
        "small_data = small_len_data.loc[:, [\"eng_text\", \"hindi_text\"]].sample(n=100000)\n",
        "train_set_sm, val_set_sm = train_test_split(small_data, test_size=0.3)\n",
        "train_set_sm.to_csv(\"/content/drive/MyDrive/ssmt/train_sm.csv\", index=False)\n",
        "val_set_sm.to_csv(\"/content/drive/MyDrive/ssmt/val_sm.csv\", index=False)"
      ],
      "metadata": {
        "id": "em5O8OQcWmcQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python==3.7\n",
        "# !pip install torch==1.6.0\n",
        "!pip install torchtext==0.6.0\n",
        "# !pip install indic-nlp-library==0.6\n",
        "# !pip install spacy==2.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O5kP2WvWmoo",
        "outputId": "95661822-fe87-4543-ed43-e9da33d1aef7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.17.1\n",
            "    Uninstalling torchtext-0.17.1:\n",
            "      Successfully uninstalled torchtext-0.17.1\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X6DFvsNcxZg",
        "outputId": "658a122d-1e4c-4e25-d828-0cb2686c69f1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install indic-nlp-library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_arxZtAUZdQv",
        "outputId": "62b8ec57-2b6e-41cc-ed12-329295b9ac6e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n",
            "Installing collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from indicnlp import common\n",
        "\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=r\"indic_nlp_resources\"\n",
        "\n",
        "# Add library to Python path\n",
        "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "# Set environment variable for resources folder\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)"
      ],
      "metadata": {
        "id": "P-eqBudXao2T"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "indic_string=\"\"\"तो क्या विश्व कप 2019 में मैच का बॉस टॉस है? यानी मैच में हार-जीत में \\\n",
        "टॉस की भूमिका अहम है? आप ऐसा सोच सकते हैं। विश्वकप के अपने-अपने पहले मैच में बुरी तरह हारने वाली एशिया की दो टीमों \\\n",
        "पाकिस्तान और श्रीलंका के कप्तान ने हालांकि अपने हार के पीछे टॉस की दलील तो नहीं दी, लेकिन यह जरूर कहा था कि वह एक अहम टॉस हार गए थे।\"\"\"\n",
        "\n",
        "# Split the sentence, language code \"hi\" is passed for hingi\n",
        "sentences=sentence_tokenize.sentence_split(indic_string, lang='hi')"
      ],
      "metadata": {
        "id": "PU_lSvBOat1D"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Packages for data generator & preparation\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "import spacy\n",
        "import sys\n",
        "from indicnlp import common\n",
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "metadata": {
        "id": "s_BnxMrzbQjf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"तुम्हारा नाम क्या है?\"\n",
        "lang = 'hi'\n",
        "\n",
        "tokens = indic_tokenize.trivial_tokenize(text, lang=lang)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5l5b4mRatq5",
        "outputId": "f5171a02-222a-4161-ea13-8b00cc69bd24"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['तुम्हारा', 'नाम', 'क्या', 'है', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "def tokenize_hindi(text):\n",
        "    return [tok for tok in indic_tokenize.trivial_tokenize(text)]\n"
      ],
      "metadata": {
        "id": "_k4eo_qJZuJV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Defining Field\n",
        "english_txt = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "hindi_txt = Field(tokenize=tokenize_hindi, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "# Defining Tabular Dataset\n",
        "data_fields = [('eng_text', english_txt), ('hindi_text', hindi_txt)]\n",
        "train_dt, val_dt = TabularDataset.splits(path='./', train='/content/drive/MyDrive/ssmt/train_sm.csv', validation='/content/drive/MyDrive/ssmt/val_sm.csv', format='csv', fields=data_fields)\n",
        "\n",
        "# Building word vocab\n",
        "english_txt.build_vocab(train_dt, max_size=10000, min_freq=2)\n",
        "hindi_txt.build_vocab(train_dt, max_size=10000, min_freq=2)"
      ],
      "metadata": {
        "id": "bGQY5NPwZvqp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "save_model = True\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 3e-4\n",
        "batch_size = 256\n",
        "\n",
        "# Defining Iterator\n",
        "train_iter = BucketIterator(train_dt, batch_size=batch_size, sort_key=lambda x: len(x.eng_text), shuffle=True)\n",
        "val_iter = BucketIterator(val_dt, batch_size=batch_size, sort_key=lambda x: len(x.eng_text), shuffle=True)\n"
      ],
      "metadata": {
        "id": "JMCj2fCRZrjG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "        \"\"\" Dividing word's embedding into 'H' different heads\n",
        "            For ex: embed_size = 512 & heads = 8\n",
        "            Then 8 heads of 64 size are created\n",
        "        \"\"\"\n",
        "        assert (self.embed_size % self.heads == 0), \"Embed size should be in multiple of heads\"\n",
        "\n",
        "        # self.values = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
        "        # self.keys = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
        "        # self.queries = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
        "        # self.fc_out = nn.Linear(in_features=self.head_dim*self.heads, out_features=self.embed_size)\n",
        "\n",
        "        self.values = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
        "        self.keys = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
        "        self.queries = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(in_features=self.head_dim * self.heads, out_features=self.embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask=None):\n",
        "        N = query.shape[0]  # Number of training examples\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Splitting embeddings into 'H' heads for creating multi-head attention\n",
        "        # V, K, Q reshape = num_samp, seq_len, heads, heads_dim\n",
        "\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        # Attention calculation\n",
        "        # attention score = softmax(Q*t(K))/sqrt(Q.shape[-1])\n",
        "        score = torch.einsum(\"nqhd,nkhd -> nhqk\", queries, keys)\n",
        "        \"\"\"\n",
        "        einsum explained: \"nqhd,nkhd -> nhqk\"\n",
        "        1. nqhd -> nhqd : queries.transpose(-2,-3) & nkhd -> nhkd : keys.transpose(-2,-3)\n",
        "        2. nhqk : (torch.bmm(nhqd.reshape(n*h,q,d), nhkd.reshape(n*h,k,d).transpose(-1,-2)).reshape(n,h,q,k)\n",
        "        \"\"\"\n",
        "\n",
        "        if mask is not None:\n",
        "            \"\"\"\n",
        "            Masking is very critical for implementing decoder side self attention\n",
        "            Since in decoding side we want to have attention scores with previous time steps elements only\n",
        "            So for this we use upper triangular masked matrix\n",
        "            \"\"\"\n",
        "            score = score.masked_fill(mask == 0, float('-1e20'))\n",
        "        attention_score = torch.softmax(score / math.sqrt(self.head_dim), dim=-1)  # N, heads, query_len, key_len\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", attention_score, values).reshape(N, query_len, self.heads*self.head_dim)\n",
        "        # out.shape >> N, query_len, embed_size\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size=embed_size, heads=heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)  # Normalization for each example for each embed dim across seq_len\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_fwd = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask=None):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))  # Layernorm1 + Skip connection\n",
        "        forward = self.feed_fwd(x)\n",
        "        out = self.dropout(self.norm2(forward + x))  # Layernorm2 + Skip connection\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    PE(pos,2i) = sin(pos/10000^(2i/emb_size))\n",
        "    PE(cos,2i+1) = cos(pos/10000^(2i+1/emb_size))\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, embed_size, dropout, device):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)  # column data : [max_len, 1]\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even place in emb_dim get sin wavelength\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd place in emb_dim get cos wavelength\n",
        "        pe = pe.unsqueeze(0).to(device)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Action Replay : Addition of position encoding with word embedding\n",
        "# Embedding output\n",
        "    vocab_size = 20\n",
        "    n_exmp = 4\n",
        "    batch_max_len = 3\n",
        "    max_len = 5\n",
        "    d_model = 10\n",
        "    x_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    x_inp = torch.randint(high=vocab_size, size=(n_exmp, batch_max_len))\n",
        "    # x_inp.shape >> torch.Size([4, 3])\n",
        "    x = x_embedding(x_inp)\n",
        "    # x.shape >> torch.Size([4, 3, 10])\n",
        "\n",
        "\n",
        "# Positional Encoding part\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    # pe.shape >>torch.Size([5, 10])\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    # position.shape >> torch.Size([5, 1])\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "    # div_term.shape >>torch.Size([5])\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    # pe.shape >>torch.Size([1, 5, 10])\n",
        "\n",
        "# Addition with embedding\n",
        "    x = x + pe[:, :x.size(1), :]\n",
        "    # pe[:, :x.size(1), :].shape >> torch.Size([1, 3, 10])\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(embed_size=embed_size,\n",
        "                                                  heads=heads,\n",
        "                                                  dropout=dropout,\n",
        "                                                  forward_expansion=forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, trg_mask, src_mask=None):\n",
        "        attention = self.attention(x, x, x, mask=trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))  # LayerNorm + Skip connection\n",
        "        out = self.transformer_block(value=value, key=key, query=query, mask=src_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "HtEqZLQOeI7W"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = PositionalEncoding(max_len, embed_size, dropout, device=self.device)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(embed_size=embed_size,\n",
        "                                 heads=heads,\n",
        "                                 dropout=dropout,\n",
        "                                 forward_expansion=forward_expansion)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.word_embedding(x)\n",
        "        out = self.position_embedding(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(value=out, key=out, query=out, mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = PositionalEncoding(max_len, embed_size, dropout, device=self.device)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size=embed_size,\n",
        "                             heads=heads,\n",
        "                             dropout=dropout,\n",
        "                             forward_expansion=forward_expansion,\n",
        "                             device=self.device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "\n",
        "    def forward(self, x, enc_out, trg_mask, src_mask=None):\n",
        "        x = self.word_embedding(x)\n",
        "        x = self.position_embedding(x)\n",
        "\n",
        "        \"\"\"\n",
        "        In decoder part key & value comes from the encoder output\n",
        "        while query comes from the self attention layer's output of the decoder\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x=x, value=enc_out, key=enc_out, trg_mask=trg_mask, src_mask=src_mask)\n",
        "\n",
        "        dec_out = self.fc_out(x)\n",
        "        return dec_out"
      ],
      "metadata": {
        "id": "2UrMn4PteEPA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 trg_vocab_size,\n",
        "                 src_pad_idx,\n",
        "                 trg_pad_idx,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 forward_expansion,\n",
        "                 heads,\n",
        "                 dropout,\n",
        "                 device=\"cuda\",\n",
        "                 max_len=500):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size=src_vocab_size,\n",
        "                               embed_size=embed_size,\n",
        "                               num_layers=num_layers,\n",
        "                               heads=heads,\n",
        "                               device=device,\n",
        "                               forward_expansion=forward_expansion,\n",
        "                               dropout=dropout,\n",
        "                               max_len=max_len)\n",
        "\n",
        "        self.decoder = Decoder(trg_vocab_size=trg_vocab_size,\n",
        "                               embed_size=embed_size,\n",
        "                               num_layers=num_layers,\n",
        "                               heads=heads,\n",
        "                               device=device,\n",
        "                               forward_expansion=forward_expansion,\n",
        "                               dropout=dropout,\n",
        "                               max_len=max_len)\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len)))\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src=src)\n",
        "        trg_mask = self.make_trg_mask(trg=trg)\n",
        "        enc_src = self.encoder(x=src, mask=src_mask)\n",
        "        out = self.decoder(x=trg, enc_out=enc_src, trg_mask=trg_mask, src_mask=src_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "tFF8jrPmeDjy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model hyper-parameters\n",
        "src_vocab_size = len(english_txt.vocab)\n",
        "trg_vocab_size = len(hindi_txt.vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_layers = 3\n",
        "dropout = 0.10\n",
        "max_len = 10000\n",
        "forward_expansion = 4\n",
        "src_pad_idx = english_txt.vocab.stoi[\"<pad>\"]\n",
        "trg_pad_idx = 0\n",
        "\n",
        "# Defining model & optimizer attributes\n",
        "model = Transformer(src_vocab_size=src_vocab_size,\n",
        "                    trg_vocab_size=trg_vocab_size,\n",
        "                    src_pad_idx=src_pad_idx,\n",
        "                    trg_pad_idx=trg_pad_idx,\n",
        "                    embed_size=embedding_size,\n",
        "                    num_layers=num_layers,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                    heads=num_heads,\n",
        "                    dropout=dropout,\n",
        "                    device=device,\n",
        "                    max_len=max_len).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "pad_idx = hindi_txt.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "loss_tracker = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPTiXE00Zo_4",
        "outputId": "96d078e1-5e4f-4841-89d9-5d45ca3a358e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename=\"/content/drive/MyDrive/ssmt/my_checkpoint_1.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer=None):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "sRIlhSZzehsz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/ssmt/my_checkpoint.pth\"\n",
        "model_1 = Transformer(src_vocab_size=src_vocab_size,\n",
        "                    trg_vocab_size=trg_vocab_size,\n",
        "                    src_pad_idx=src_pad_idx,\n",
        "                    trg_pad_idx=trg_pad_idx,\n",
        "                    embed_size=embedding_size,\n",
        "                    num_layers=num_layers,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                    heads=num_heads,\n",
        "                    dropout=dropout,\n",
        "                    device=device,\n",
        "                    max_len=max_len)\n",
        "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "model_dict = model_1.state_dict()\n",
        "pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in model_dict}\n",
        "model_dict.update(pretrained_dict)\n",
        "model_1.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "# model = load_checkpoint(checkpoint_path, model, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Toy0moDMM3ma",
        "outputId": "31527fe8-0320-43c6-cce4-c3ba14338b06"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    loop = tqdm(enumerate(train_iter), total=len(train_iter))\n",
        "    for batch_idx, batch in loop:\n",
        "        # Get input and targets and move to GPU if available\n",
        "        # Switching axis because bucket-iterator gives output of size(seq_len,bs)\n",
        "        inp_data = batch.eng_text.permute(-1, -2).to(device)\n",
        "        target = batch.hindi_text.permute(-1, -2).to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        output = model(inp_data, target[:, :-1])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output.reshape(-1, trg_vocab_size), target[:, 1:].reshape(-1))\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Checking GPU uses\n",
        "        if device.type == \"cuda\":\n",
        "            total_mem = torch.cuda.get_device_properties(0).total_memory/1024/1024\n",
        "            allocated_mem = torch.cuda.memory_allocated(0)/1024/1024\n",
        "            reserved_mem = torch.cuda.memory_reserved(0)/1024/1024\n",
        "        else:\n",
        "            total_mem = 0\n",
        "            allocated_mem = 0\n",
        "            reserved_mem = 0\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Clipping exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_postfix(loss=loss.item(), total_gpu_mem=str(total_mem), gpu_allocated_mem=str(allocated_mem), gpu_reserved_mem=str(reserved_mem))\n",
        "        # break\n",
        "    train_mean_loss = sum(losses) / len(losses)\n",
        "    scheduler.step(train_mean_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for val_batch_idx, val_batch in tqdm(enumerate(val_iter), total=len(val_iter)):\n",
        "            val_inp_data = val_batch.eng_text.permute(-1, -2).to(device)\n",
        "            val_target = val_batch.hindi_text.permute(-1, -2).to(device)\n",
        "            val_output = model(val_inp_data, val_target[:, :-1])\n",
        "            val_loss = criterion(val_output.reshape(-1, trg_vocab_size), val_target[:, 1:].reshape(-1))\n",
        "            val_losses.append(val_loss.item())\n",
        "        val_mean_loss = sum(val_losses)/len(val_losses)\n",
        "\n",
        "    loss_tracker.append(val_mean_loss)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        if save_model and val_mean_loss == np.min(loss_tracker):\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}]: train_loss= {train_mean_loss}; val_loss= {val_mean_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-jAr0wGZmQx",
        "outputId": "8646d69f-929b-45ba-b0c7-5a522a542cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 274/274 [02:58<00:00,  1.54it/s, gpu_allocated_mem=4247.82861328125, gpu_reserved_mem=12438.0, loss=4.74, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [1/10]: train_loss= 5.556659867293643; val_loss= 4.543299707315736\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 274/274 [03:09<00:00,  1.45it/s, gpu_allocated_mem=5081.4560546875, gpu_reserved_mem=12438.0, loss=4.04, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.83it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [2/10]: train_loss= 4.330649304563981; val_loss= 3.842501684770746\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 274/274 [03:10<00:00,  1.44it/s, gpu_allocated_mem=4867.3935546875, gpu_reserved_mem=12438.0, loss=3.6, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [3/10]: train_loss= 3.758851960627702; val_loss= 3.4006275868011735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [03:10<00:00,  1.43it/s, gpu_allocated_mem=4676.083984375, gpu_reserved_mem=13054.0, loss=3.16, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [4/10]: train_loss= 3.3401954278458645; val_loss= 3.0735124491028865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [03:10<00:00,  1.44it/s, gpu_allocated_mem=5353.9912109375, gpu_reserved_mem=13054.0, loss=2.97, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [5/10]: train_loss= 3.0092398711364634; val_loss= 2.808215612072056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [03:10<00:00,  1.43it/s, gpu_allocated_mem=4805.9580078125, gpu_reserved_mem=13054.0, loss=2.69, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [6/10]: train_loss= 2.729782407301186; val_loss= 2.6208707659931507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [03:10<00:00,  1.44it/s, gpu_allocated_mem=4963.05859375, gpu_reserved_mem=13054.0, loss=2.47, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [7/10]: train_loss= 2.4984365388424727; val_loss= 2.4483918557732793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [03:10<00:00,  1.44it/s, gpu_allocated_mem=4782.244140625, gpu_reserved_mem=13054.0, loss=2.08, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [8/10]: train_loss= 2.303761962556491; val_loss= 2.3069613101118702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [03:10<00:00,  1.44it/s, gpu_allocated_mem=5068.0908203125, gpu_reserved_mem=13054.0, loss=1.9, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [9/10]: train_loss= 2.1390210372688125; val_loss= 2.1886467569965427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [03:11<00:00,  1.43it/s, gpu_allocated_mem=4990.611328125, gpu_reserved_mem=13054.0, loss=2.12, total_gpu_mem=15102.0625]\n",
            "100%|██████████| 118/118 [00:30<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch [10/10]: train_loss= 1.9925048721097682; val_loss= 2.0887534446635487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = {\n",
        "#     \"state_dict\": model.state_dict(),\n",
        "#     \"optimizer\": optimizer.state_dict(),\n",
        "# }\n",
        "\n",
        "# save_checkpoint(checkpoint)"
      ],
      "metadata": {
        "id": "PR-Hz-ssHg6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings generated"
      ],
      "metadata": {
        "id": "p1kZeXjikb9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_word_embeddings = model.encoder.word_embedding\n",
        "trg_word_embeddings = model.decoder.word_embedding"
      ],
      "metadata": {
        "id": "wc_azUsVkdxX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# src_word_embeddings = model.encoder.word_embedding\n",
        "# torch.save(src_word_embeddings.state_dict(), '/content/drive/MyDrive/ssmt/src_word_embeddings.pth')\n",
        "\n",
        "# trg_word_embeddings = model.encoder.word_embedding\n",
        "# torch.save(trg_word_embeddings.state_dict(), '/content/drive/MyDrive/ssmt/trg_word_embeddings.pth')"
      ],
      "metadata": {
        "id": "91FfcTbgnBhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "src_word_embeddings = nn.Embedding(src_vocab_size, 512)\n",
        "src_word_embeddings.load_state_dict(torch.load('/content/drive/MyDrive/ssmt/src_word_embeddings.pth'))\n",
        "src_word_embeddings.to(device)\n",
        "# print(src_word_embeddings)\n",
        "\n",
        "trg_word_embeddings = nn.Embedding(src_vocab_size, 512)\n",
        "trg_word_embeddings.load_state_dict(torch.load('/content/drive/MyDrive/ssmt/trg_word_embeddings.pth'))\n",
        "trg_word_embeddings.to(device)\n",
        "# print(trg_word_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrtfkyfKnIu7",
        "outputId": "6c7efa2c-0691-4e57-cedd-a34d482b8655"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(10004, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = \"hello\"\n",
        "token_idx = english_txt.vocab.stoi[token]\n",
        "embedding = src_word_embeddings(torch.LongTensor([token_idx]).to(src_word_embeddings.weight.device))\n",
        "print(embedding.shape)\n"
      ],
      "metadata": {
        "id": "5AC0GGy6gsMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d30fce-15df-4ac0-e14a-3f82067e550a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "4zOS7ygke23f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = model_1.to(\"cuda\")"
      ],
      "metadata": {
        "id": "nMRXyPkhomot"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(sentence, model, src_field, trg_field, k, max_len=50, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    model = model.to(device)  # Move model to the specified device\n",
        "\n",
        "    # Tokenize the input sentence\n",
        "    tokens = src_field.tokenize(sentence)\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize the beam\n",
        "    beams = [(src_tensor, [trg_field.vocab.stoi[trg_field.init_token]], 0)]\n",
        "\n",
        "    # Start beam search\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        end_generation = False\n",
        "        for src, trg, score in beams:\n",
        "            trg_tensor = torch.LongTensor(trg).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model(src, trg_tensor)\n",
        "            probs = torch.softmax(output, dim=-1)[0, -1, :]\n",
        "            topk_probs, topk_idx = torch.topk(probs, k)\n",
        "\n",
        "            for i in range(k):\n",
        "                new_trg = trg + [topk_idx[i].item()]\n",
        "                new_score = score + np.log(topk_probs[i].item())\n",
        "                new_beams.append((src, new_trg, new_score))\n",
        "                if topk_idx[i].item() == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                    end_generation = True\n",
        "                    break\n",
        "\n",
        "            if end_generation:\n",
        "                break\n",
        "\n",
        "        if end_generation:\n",
        "            break\n",
        "\n",
        "        beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:k]\n",
        "\n",
        "    # Select the best beam\n",
        "    best_beam = beams[0]\n",
        "    translation = [trg_field.vocab.itos[idx] for idx in best_beam[1]][1:-1]  # Remove <sos> and <eos> tokens\n",
        "    translation = \" \".join(translation)\n",
        "\n",
        "    return translation\n",
        "\n",
        "# Example usage\n",
        "sentence = \"right though they do not have this equalitysign explicitly implicitly this is assigning the new values for f as we step the list cfright\"\n",
        "translation = beam_search(sentence, model_1, src_field=english_txt, trg_field=hindi_txt, k=5, max_len=50, device=\"cuda\")\n",
        "print(translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUVMWZLRrCm8",
        "outputId": "bd34d9dd-5e5d-4d16-fd09-30e9aa5aa19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> इस प्रकार वे इस प्रकार नहीं हैं कि यह एक सामान्य मूल्य है जैसा कि सूची के रूप में नए मूल्यों के लिए हम सामान्य रूप\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(sentence, model, src_field, trg_field, k, max_len=50, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    model = model.to(device)  # Move model to the specified device\n",
        "\n",
        "    # Tokenize the input sentence\n",
        "    tokens = src_field.tokenize(sentence)\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize the beam\n",
        "    beams = [(src_tensor, [trg_field.vocab.stoi[trg_field.init_token]], 0)]\n",
        "\n",
        "    # Start beam search\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        end_generation = False\n",
        "        for src, trg, score in beams:\n",
        "            trg_tensor = torch.LongTensor(trg).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model(src, trg_tensor)\n",
        "            probs = torch.softmax(output, dim=-1)[0, -1, :]\n",
        "            topk_probs, topk_idx = torch.topk(probs, k)\n",
        "\n",
        "            for i in range(k):\n",
        "                new_trg = trg + [topk_idx[i].item()]\n",
        "                new_score = score + np.log(topk_probs[i].item())\n",
        "                new_beams.append((src, new_trg, new_score))\n",
        "                if topk_idx[i].item() == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                    end_generation = True\n",
        "                    break\n",
        "\n",
        "            if end_generation:\n",
        "                break\n",
        "\n",
        "        if end_generation:\n",
        "            break\n",
        "\n",
        "        beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:k]\n",
        "\n",
        "    # Select the best beam\n",
        "    best_beam = beams[0]\n",
        "    translation = [trg_field.vocab.itos[idx] for idx in best_beam[1]][1:-1]  # Remove <sos> and <eos> tokens\n",
        "    translation = \" \".join(translation)\n",
        "\n",
        "    return translation\n",
        "\n",
        "# Example usage\n",
        "sentence = \"right though they do not have this equalitysign explicitly implicitly this is assigning the new values for f as we step the list cfright\"\n",
        "translation = beam_search(sentence, model_1, src_field=english_txt, trg_field=hindi_txt, k=5, max_len=50, device=\"cuda\")\n",
        "print(translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWg4bsoQVYRa",
        "outputId": "8fc9d1d3-4666-4ca9-89d4-49a956a9cc4e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "और ऐ रब अंत की रूप संपत्ति मेरे को तुम्हारे क्या ने दाख़िल तो सामने स्त्रियों मार्ग थे कि वह लेते कुरान रोज़ी पर\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING WORD2VEc Embeddings"
      ],
      "metadata": {
        "id": "JfaNSbuQ1p6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# # Ensure NLTK is downloaded\n",
        "# nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "nltk.data.path.append(\"/content/nltk_data/\")\n",
        "nltk.download('punkt', download_dir=\"/content/nltk_data/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peKsc3Q21zYV",
        "outputId": "13e9469c-7760-49f2-f1bb-6629605bf6b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data/...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to the English folder\n",
        "eng_folder_path = os.path.join(content, \"ssmt\", \"Eng_all\", \"Eng_all\")\n",
        "\n",
        "# List all files in the English folder\n",
        "eng_files = glob.glob(os.path.join(eng_folder_path, \"*\"))\n",
        "\n",
        "# Path to the Hindi folder\n",
        "hin_folder_path = os.path.join(content, \"ssmt\", \"Hin_all\", \"Hin_all\")\n",
        "\n",
        "# List all files in the Hindi folder\n",
        "hin_files = glob.glob(os.path.join(hin_folder_path, \"*\"))\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "eng_directory = '/content/drive/MyDrive/ssmt/Eng_all/Eng_all/'\n",
        "eng_files = os.listdir(eng_directory)\n",
        "\n",
        "\n",
        "hin_directory = '/content/drive/MyDrive/ssmt/Hin_all/Hin_all/'\n",
        "hin_files = os.listdir(hin_directory)\n",
        "\n",
        "# Read the content of each English file, convert to lowercase, and strip whitespace\n",
        "eng_file_contents = {}\n",
        "hin_file_contents = {}\n",
        "\n",
        "for file_name in hin_files:\n",
        "    eng_file_path = os.path.join(eng_directory, file_name)\n",
        "    hin_file_path = os.path.join(hin_directory, file_name)\n",
        "    if os.path.isfile(eng_file_path):\n",
        "        with open(eng_file_path, 'r') as file:\n",
        "            lines = [line.lower().strip() for line in file.readlines()]\n",
        "            eng_file_contents[file_name] = lines\n",
        "\n",
        "    if os.path.isfile(hin_file_path):\n",
        "        with open(hin_file_path, 'r') as file:\n",
        "            lines = [line.lower().strip() for line in file.readlines()]\n",
        "            hin_file_contents[file_name] = lines\n",
        "# Remove the specified key from the dictionaries\n",
        "hin_file_contents.pop(\"algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt\", None)\n",
        "eng_file_contents.pop(\"algorithms and programming： simple gcd [9MmC_uGjBsM].en.txt\", None)\n",
        "\n",
        "\n",
        "sentences_hindi = []\n",
        "for file_name in hin_file_contents.keys():\n",
        "    for sentence in hin_file_contents[file_name]:\n",
        "        sentences_hindi.append(sentence)\n",
        "\n",
        "sentences_english = []\n",
        "for file_name in eng_file_contents.keys():\n",
        "    for sentence in eng_file_contents[file_name]:\n",
        "        sentences_english.append(sentence)"
      ],
      "metadata": {
        "id": "hqW4EtLnF6mL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "\n",
        "print(dataset.keys())\n",
        "print(dataset['train'])\n",
        "print(len(dataset['train'][\"translation\"][0]))\n",
        "print(dataset['train'][\"translation\"][0])\n",
        "\n",
        "for translation_pair in dataset[\"train\"][\"translation\"][:500000]:\n",
        "  sentences_hindi.append(translation_pair[\"hi\"])\n",
        "  sentences_english.append(translation_pair[\"en\"])\n",
        "\n",
        "hindi_text = sentences_hindi\n",
        "eng_text = sentences_english"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T0e765QGgUg",
        "outputId": "d1b408d2-b06d-4a5e-e666-09ff1d1db597"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['train', 'validation', 'test'])\n",
            "Dataset({\n",
            "    features: ['translation'],\n",
            "    num_rows: 1659083\n",
            "})\n",
            "2\n",
            "{'en': 'Give your application an accessibility workout', 'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install torchtext==0.6.0\n",
        "!pip install indic-nlp-library\n",
        "\n",
        "\n",
        "import sys\n",
        "from indicnlp import common\n",
        "\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=r\"indic_nlp_resources\"\n",
        "\n",
        "# Add library to Python path\n",
        "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "# Set environment variable for resources folder\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "# Basic packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Packages for data generator & preparation\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "import spacy\n",
        "import sys\n",
        "from indicnlp import common\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "# Data Prep\n",
        "# Settings for handling english text\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Defining Tokenizer\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "\n",
        "def tokenize_hindi(text):\n",
        "    return [tok for tok in indic_tokenize.trivial_tokenize(text)]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RCpX23aHrlX",
        "outputId": "28d773c3-1be5-44d5-addb-4b239ee83853"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\n",
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.4.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create vocabulary\n",
        "def create_vocab_eng(sents):\n",
        "  vocab = dict()\n",
        "\n",
        "  vocab[\"<pad>\"] = 0\n",
        "  vocab[\"<sos>\"] = 1\n",
        "  vocab[\"<eos>\"] = 2\n",
        "  vocab[\"<unk>\"] = 3\n",
        "\n",
        "  for sent in sents:\n",
        "    words = word_tokenize(sent)\n",
        "    for word in words:\n",
        "      if word not in vocab:\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def create_vocab_hin(sents):\n",
        "  vocab = dict()\n",
        "\n",
        "  vocab[\"<pad>\"] = 0\n",
        "  vocab[\"<sos>\"] = 1\n",
        "  vocab[\"<eos>\"] = 2\n",
        "  vocab[\"<unk>\"] = 3\n",
        "\n",
        "  for sent in sents:\n",
        "    words = indic_tokenize.trivial_tokenize(sent, lang='hi')\n",
        "    for word in words:\n",
        "      if word not in vocab:\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "\n",
        "  return vocab\n",
        "\n",
        "# do processing and create vocab source and target vocab\n",
        "src_vocab = create_vocab_eng(sentences_english)\n",
        "target_vocab = create_vocab_hin(sentences_hindi)\n",
        "\n",
        "print(len(src_vocab), len(target_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh49m5rNr9hm",
        "outputId": "8df8e70a-3aec-41f7-8dd5-71008fe90df0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89314 100895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_inv = {v: k for k, v in src_vocab.items()}\n",
        "target_vocab_inv = {v: k for k, v in target_vocab.items()}"
      ],
      "metadata": {
        "id": "7sFbWUre2HF5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_hindi[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1YaEsG5s3BUo",
        "outputId": "197c1e4a-06de-462d-a990-1484302e173b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'व्याख्यान में, हमने देखा कि ऑब्जेक्ट ओरिएंटेड प्रोग्रामिंग में हम एक डेटा प्रकार को एक टेम्पलेट के माध्यम से परिभाषित करते हैं जिसे एक वर्ग कहा जाता है, जो आंतरिक डेटा कार्यान्वयन और उन कार्यों को परिभाषित करता है जिनका उपयोग हम डेटा प्रकार में हेरफेर करने के लिए करते हैं।'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_hindis = []\n",
        "for file_name in hin_file_contents.keys():\n",
        "    for sentence in hin_file_contents[file_name]:\n",
        "        sentences_hindis.append(['<sos>'] + sentence.split() + ['<eos>'])\n",
        "\n",
        "\n",
        "sentences_englishs = []\n",
        "for file_name in eng_file_contents.keys():\n",
        "    for sentence in eng_file_contents[file_name]:\n",
        "        sentences_englishs.append(['<sos>'] + sentence.split() + ['<eos>'])"
      ],
      "metadata": {
        "id": "lg-Q6szN3Gip"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_hindis[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgjQ5pU_3PHl",
        "outputId": "efad56d1-fc20-49ee-e52e-18b505171088"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>',\n",
              " 'व्याख्यान',\n",
              " 'में,',\n",
              " 'हमने',\n",
              " 'देखा',\n",
              " 'कि',\n",
              " 'ऑब्जेक्ट',\n",
              " 'ओरिएंटेड',\n",
              " 'प्रोग्रामिंग',\n",
              " 'में',\n",
              " 'हम',\n",
              " 'एक',\n",
              " 'डेटा',\n",
              " 'प्रकार',\n",
              " 'को',\n",
              " 'एक',\n",
              " 'टेम्पलेट',\n",
              " 'के',\n",
              " 'माध्यम',\n",
              " 'से',\n",
              " 'परिभाषित',\n",
              " 'करते',\n",
              " 'हैं',\n",
              " 'जिसे',\n",
              " 'एक',\n",
              " 'वर्ग',\n",
              " 'कहा',\n",
              " 'जाता',\n",
              " 'है,',\n",
              " 'जो',\n",
              " 'आंतरिक',\n",
              " 'डेटा',\n",
              " 'कार्यान्वयन',\n",
              " 'और',\n",
              " 'उन',\n",
              " 'कार्यों',\n",
              " 'को',\n",
              " 'परिभाषित',\n",
              " 'करता',\n",
              " 'है',\n",
              " 'जिनका',\n",
              " 'उपयोग',\n",
              " 'हम',\n",
              " 'डेटा',\n",
              " 'प्रकार',\n",
              " 'में',\n",
              " 'हेरफेर',\n",
              " 'करने',\n",
              " 'के',\n",
              " 'लिए',\n",
              " 'करते',\n",
              " 'हैं।',\n",
              " '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model_hindi = Word2Vec(sentences_hindis, vector_size=512, window=5, min_count=1, sg=0)\n",
        "word2vec_model_hindi = word2vec_model_hindi.wv\n",
        "\n",
        "print(word2vec_model_hindi.index_to_key[0])\n",
        "\n",
        "word2vec_model_hindi.index_to_key.insert(0, '<pad>')\n",
        "word2vec_model_hindi.vectors = np.insert(word2vec_model_hindi.vectors, 0, np.zeros(512), axis=0)\n",
        "word2vec_model_hindi.key_to_index = {word: index for index, word in enumerate(word2vec_model_hindi.index_to_key)}\n",
        "\n",
        "\n",
        "next_index = len(word2vec_model_hindi.key_to_index)\n",
        "word2vec_model_hindi.index_to_key.insert(next_index, '<unk>')\n",
        "word2vec_model_hindi.vectors = np.insert(word2vec_model_hindi.vectors, next_index, np.random.rand(512), axis=0)\n",
        "word2vec_model_hindi.key_to_index = {word: index for index, word in enumerate(word2vec_model_hindi.index_to_key)}\n",
        "\n",
        "# print(word2vec_model_hindi.key_to_index['<pad>'])\n",
        "print(word2vec_model_hindi.key_to_index['<unk>'])\n",
        "print(word2vec_model_hindi.index_to_key[0])\n",
        "print(word2vec_model_hindi.index_to_key[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V7YmYa31tfe",
        "outputId": "468ab685-8832-43fe-8057-cfd5d3e44105"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eos>\n",
            "5231\n",
            "<pad>\n",
            "<eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "word2vec_model_english = Word2Vec(sentences_englishs, vector_size=512, window=5, min_count=1, sg=0)\n",
        "word2vec_model_english = word2vec_model_english.wv\n",
        "print(word2vec_model_english.key_to_index['the'])\n",
        "\n",
        "word2vec_model_english.index_to_key.insert(0, '<pad>')\n",
        "word2vec_model_english.vectors = np.insert(word2vec_model_english.vectors, 0, np.zeros(512), axis=0)\n",
        "word2vec_model_english.key_to_index = {word: index for index, word in enumerate(word2vec_model_english.index_to_key)}\n",
        "\n",
        "\n",
        "next_index = len(word2vec_model_english.key_to_index)\n",
        "word2vec_model_english.index_to_key.insert(next_index, '<unk>')\n",
        "word2vec_model_english.vectors = np.insert(word2vec_model_english.vectors, next_index, np.random.rand(512), axis=0)\n",
        "word2vec_model_english.key_to_index = {word: index for index, word in enumerate(word2vec_model_english.index_to_key)}\n",
        "\n",
        "print(word2vec_model_english.key_to_index['<pad>'])\n",
        "print(word2vec_model_english.key_to_index['<unk>'])\n",
        "print(word2vec_model_english.key_to_index['the'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3XbFnTT2pF1",
        "outputId": "1e5b7ac8-d234-421c-e819-a3b05463630f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "6387\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# import copy\n",
        "\n",
        "\n",
        "# # Importing contractions\n",
        "# with open(\"/content/drive/MyDrive/ssmt/contractions.txt\", \"r\") as inp_cont:\n",
        "#     contractions_list = inp_cont.read()\n",
        "# contractions_list = [re.sub('[\"]', '', x).split(\":\") for x in re.sub(r\"\\s+\", \" \", re.sub(r\"(.*{)|(}.*)\", '', contractions_list)).split(',')]\n",
        "# contractions_dict = dict((k.lower().strip(), re.sub('/.*', '', v).lower().strip()) for k, v in contractions_list)\n",
        "\n",
        "# def remove_sc(_line, lang=\"en\"):\n",
        "#     # _line = copy.deepcopy(_line)\n",
        "#     if lang == \"hi\":\n",
        "#         _line = re.sub(r'[+\\-*/#@%>=;~{}×–`’\"()_]', \"\", _line)\n",
        "#         _line = re.sub(r\"(?:(\\[)|(\\])|(‘‘)|(’’))\", '', _line)\n",
        "#     elif lang == \"en\":\n",
        "#         _line = re.sub(r'[+\\-*/#@%>=;~{}×–`’\"()_|:]', \"\", _line)\n",
        "#         _line = re.sub(r\"(?:(\\[)|(\\])|(‘‘)|(’’))\", '', _line)\n",
        "#     return _line\n",
        "\n",
        "\n",
        "# def clean_text(_text, lang=\"en\"):\n",
        "#     # _text = copy.deepcopy(_text)\n",
        "#     if lang == \"en\":\n",
        "#         _text = remove_sc(_line=_text, lang=lang)\n",
        "#         for cn in contractions_dict:\n",
        "#             _text = re.sub(cn, contractions_dict[cn], _text)\n",
        "#     elif lang == \"hi\":\n",
        "#         _text = remove_sc(_line=_text, lang=lang)\n",
        "#     return _text\n",
        "# # Removing Hindi sentences having english letter in it\n",
        "# ids_to_remove = {}\n",
        "# for _id, _t in tqdm(enumerate(hindi_text)):\n",
        "#     if len(re.findall(r'[a-zA-Z]', _t)) > 0:\n",
        "#         ids_to_remove[_id] = _t\n",
        "#         # ids_to_remove.append(_id)\n",
        "#     else:\n",
        "#         pass\n",
        "\n",
        "\n",
        "# ids_to_keep = [i for i in range(len(hindi_text)) if i not in ids_to_remove.keys()]\n",
        "# filtered_eng_text = []\n",
        "# filtered_hindi_text = []\n",
        "# for _id in tqdm(ids_to_keep):\n",
        "#     filtered_eng_text.append(eng_text[_id].lower())\n",
        "#     filtered_hindi_text.append(hindi_text[_id])\n",
        "\n",
        "\n",
        "# # Treating english sentences\n",
        "# clean_eng_text = []\n",
        "# for sent in tqdm(filtered_eng_text):\n",
        "#     clean_eng_text.append(clean_text(_text=copy.deepcopy(sent), lang=\"en\"))\n",
        "\n",
        "\n",
        "# # Treating hindi sentences\n",
        "# clean_hindi_text = []\n",
        "# for sent in tqdm(filtered_hindi_text):\n",
        "#     clean_hindi_text.append(clean_text(_text=copy.deepcopy(sent), lang=\"hi\"))\n",
        "\n",
        "\n",
        "\n",
        "# # Filtered Data\n",
        "# clean_data = pd.DataFrame({\"eng_text\": clean_eng_text, \"hindi_text\": clean_hindi_text})\n",
        "\n",
        "\n",
        "# # Filtering data based on sentence length\n",
        "# clean_data[\"eng_len\"] = clean_data.eng_text.str.count(\" \")\n",
        "# clean_data[\"hindi_len\"] = clean_data.hindi_text.str.count(\" \")\n",
        "# small_len_data = clean_data.query('eng_len < 50 & hindi_len < 50')"
      ],
      "metadata": {
        "id": "snPyXkWQHMiz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model_english.vectors.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZATQPhVXR-B",
        "outputId": "d1b13b2b-5b0d-4096-e68c-6786c13356f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6388"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Field\n",
        "english_txt = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "hindi_txt = Field(tokenize=tokenize_hindi, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "# Defining Tabular Dataset\n",
        "data_fields = [('eng_text', english_txt), ('hindi_text', hindi_txt)]\n",
        "train_dt, val_dt = TabularDataset.splits(path='./', train='/content/drive/MyDrive/ssmt/train_sm.csv', validation='/content/drive/MyDrive/ssmt/val_sm.csv', format='csv', fields=data_fields)\n",
        "\n",
        "# Building word vocab\n",
        "english_txt.build_vocab(train_dt, max_size=word2vec_model_english.vectors.shape[0]+1, min_freq=2)\n",
        "hindi_txt.build_vocab(train_dt, max_size=word2vec_model_hindi.vectors.shape[0]+1, min_freq=2)\n",
        "\n",
        "# Training & Evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "save_model = True\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 3e-4\n",
        "batch_size = 256\n",
        "\n",
        "# Defining Iterator\n",
        "train_iter = BucketIterator(train_dt, batch_size=batch_size, sort_key=lambda x: len(x.eng_text), shuffle=True)\n",
        "val_iter = BucketIterator(val_dt, batch_size=batch_size, sort_key=lambda x: len(x.eng_text), shuffle=True)"
      ],
      "metadata": {
        "id": "Jao6pxCyHxvV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "        \"\"\" Dividing word's embedding into 'H' different heads\n",
        "            For ex: embed_size = 512 & heads = 8\n",
        "            Then 8 heads of 64 size are created\n",
        "        \"\"\"\n",
        "        assert (self.embed_size % self.heads == 0), \"Embed size should be in multiple of heads\"\n",
        "\n",
        "        # self.values = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
        "        # self.keys = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
        "        # self.queries = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
        "        # self.fc_out = nn.Linear(in_features=self.head_dim*self.heads, out_features=self.embed_size)\n",
        "\n",
        "        self.values = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
        "        self.keys = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
        "        self.queries = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(in_features=self.head_dim * self.heads, out_features=self.embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask=None):\n",
        "        N = query.shape[0]  # Number of training examples\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Splitting embeddings into 'H' heads for creating multi-head attention\n",
        "        # V, K, Q reshape = num_samp, seq_len, heads, heads_dim\n",
        "\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        # Attention calculation\n",
        "        # attention score = softmax(Q*t(K))/sqrt(Q.shape[-1])\n",
        "        score = torch.einsum(\"nqhd,nkhd -> nhqk\", queries, keys)\n",
        "        \"\"\"\n",
        "        einsum explained: \"nqhd,nkhd -> nhqk\"\n",
        "        1. nqhd -> nhqd : queries.transpose(-2,-3) & nkhd -> nhkd : keys.transpose(-2,-3)\n",
        "        2. nhqk : (torch.bmm(nhqd.reshape(n*h,q,d), nhkd.reshape(n*h,k,d).transpose(-1,-2)).reshape(n,h,q,k)\n",
        "        \"\"\"\n",
        "\n",
        "        if mask is not None:\n",
        "            \"\"\"\n",
        "            Masking is very critical for implementing decoder side self attention\n",
        "            Since in decoding side we want to have attention scores with previous time steps elements only\n",
        "            So for this we use upper triangular masked matrix\n",
        "            \"\"\"\n",
        "            score = score.masked_fill(mask == 0, float('-1e20'))\n",
        "        attention_score = torch.softmax(score / math.sqrt(self.head_dim), dim=-1)  # N, heads, query_len, key_len\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", attention_score, values).reshape(N, query_len, self.heads*self.head_dim)\n",
        "        # out.shape >> N, query_len, embed_size\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size=embed_size, heads=heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)  # Normalization for each example for each embed dim across seq_len\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_fwd = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask=None):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))  # Layernorm1 + Skip connection\n",
        "        forward = self.feed_fwd(x)\n",
        "        out = self.dropout(self.norm2(forward + x))  # Layernorm2 + Skip connection\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    PE(pos,2i) = sin(pos/10000^(2i/emb_size))\n",
        "    PE(cos,2i+1) = cos(pos/10000^(2i+1/emb_size))\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, embed_size, dropout, device):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)  # column data : [max_len, 1]\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even place in emb_dim get sin wavelength\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd place in emb_dim get cos wavelength\n",
        "        pe = pe.unsqueeze(0).to(device)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Action Replay : Addition of position encoding with word embedding\n",
        "# Embedding output\n",
        "    vocab_size = 20\n",
        "    n_exmp = 4\n",
        "    batch_max_len = 3\n",
        "    max_len = 5\n",
        "    d_model = 10\n",
        "    x_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    x_inp = torch.randint(high=vocab_size, size=(n_exmp, batch_max_len))\n",
        "    # x_inp.shape >> torch.Size([4, 3])\n",
        "    x = x_embedding(x_inp)\n",
        "    # x.shape >> torch.Size([4, 3, 10])\n",
        "\n",
        "\n",
        "# Positional Encoding part\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    # pe.shape >>torch.Size([5, 10])\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    # position.shape >> torch.Size([5, 1])\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "    # div_term.shape >>torch.Size([5])\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    # pe.shape >>torch.Size([1, 5, 10])\n",
        "\n",
        "# Addition with embedding\n",
        "    x = x + pe[:, :x.size(1), :]\n",
        "    # pe[:, :x.size(1), :].shape >> torch.Size([1, 3, 10])\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(embed_size=embed_size,\n",
        "                                                  heads=heads,\n",
        "                                                  dropout=dropout,\n",
        "                                                  forward_expansion=forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, trg_mask, src_mask=None):\n",
        "        attention = self.attention(x, x, x, mask=trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))  # LayerNorm + Skip connection\n",
        "        out = self.transformer_block(value=value, key=key, query=query, mask=src_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "49XE8zyi3dn3"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        # self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        # self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model_english.vectors), freeze=True)\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(word2vec_model_english.vectors),\n",
        "            freeze=True,\n",
        "            padding_idx=english_txt.vocab.stoi[english_txt.pad_token]\n",
        "        )\n",
        "        self.position_embedding = PositionalEncoding(max_len, embed_size, dropout, device=self.device)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(embed_size=embed_size,\n",
        "                                 heads=heads,\n",
        "                                 dropout=dropout,\n",
        "                                 forward_expansion=forward_expansion)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # print(x)\n",
        "        x = self.word_embedding(x)\n",
        "        out = self.position_embedding(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(value=out, key=out, query=out, mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        # self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        # self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model_hindi.vectors), freeze=True)\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(word2vec_model_hindi.vectors),\n",
        "            freeze=True,\n",
        "            padding_idx=hindi_txt.vocab.stoi[hindi_txt.pad_token]\n",
        "        )\n",
        "        self.position_embedding = PositionalEncoding(max_len, embed_size, dropout, device=self.device)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size=embed_size,\n",
        "                             heads=heads,\n",
        "                             dropout=dropout,\n",
        "                             forward_expansion=forward_expansion,\n",
        "                             device=self.device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "\n",
        "    def forward(self, x, enc_out, trg_mask, src_mask=None):\n",
        "        x = self.word_embedding(x)\n",
        "        x = self.position_embedding(x)\n",
        "\n",
        "        \"\"\"\n",
        "        In decoder part key & value comes from the encoder output\n",
        "        while query comes from the self attention layer's output of the decoder\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x=x, value=enc_out, key=enc_out, trg_mask=trg_mask, src_mask=src_mask)\n",
        "\n",
        "        dec_out = self.fc_out(x)\n",
        "        return dec_out"
      ],
      "metadata": {
        "id": "uitCsWDTDfjA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 trg_vocab_size,\n",
        "                 src_pad_idx,\n",
        "                 trg_pad_idx,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 forward_expansion,\n",
        "                 heads,\n",
        "                 dropout,\n",
        "                 device=\"cuda\",\n",
        "                 max_len=500):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size=src_vocab_size,\n",
        "                               embed_size=embed_size,\n",
        "                               num_layers=num_layers,\n",
        "                               heads=heads,\n",
        "                               device=device,\n",
        "                               forward_expansion=forward_expansion,\n",
        "                               dropout=dropout,\n",
        "                               max_len=max_len)\n",
        "\n",
        "        self.decoder = Decoder(trg_vocab_size=trg_vocab_size,\n",
        "                               embed_size=embed_size,\n",
        "                               num_layers=num_layers,\n",
        "                               heads=heads,\n",
        "                               device=device,\n",
        "                               forward_expansion=forward_expansion,\n",
        "                               dropout=dropout,\n",
        "                               max_len=max_len)\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len)))\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src=src)\n",
        "        trg_mask = self.make_trg_mask(trg=trg)\n",
        "        enc_src = self.encoder(x=src, mask=src_mask)\n",
        "        out = self.decoder(x=trg, enc_out=enc_src, trg_mask=trg_mask, src_mask=src_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "WmlcJJ4VEf4j"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "RBY7pyuzRVex"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(english_txt.vocab)\n",
        "trg_vocab_size = len(hindi_txt.vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_layers = 3\n",
        "dropout = 0.10\n",
        "max_len = 10000\n",
        "forward_expansion = 4\n",
        "src_pad_idx = english_txt.vocab.stoi[\"<pad>\"]\n",
        "trg_pad_idx = 0\n",
        "\n",
        "# Defining model & optimizer attributes\n",
        "model_word2vec = Transformer(src_vocab_size=src_vocab_size,\n",
        "                    trg_vocab_size=trg_vocab_size,\n",
        "                    src_pad_idx=src_pad_idx,\n",
        "                    trg_pad_idx=trg_pad_idx,\n",
        "                    embed_size=embedding_size,\n",
        "                    num_layers=num_layers,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                    heads=num_heads,\n",
        "                    dropout=dropout,\n",
        "                    device=device,\n",
        "                    max_len=max_len).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model_word2vec.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "pad_idx = hindi_txt.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "loss_tracker = []"
      ],
      "metadata": {
        "id": "vg9BwyG9EkyQ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_word2vec = model_word2vec.to(device)"
      ],
      "metadata": {
        "id": "v9vBpq-oatCc"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename=\"/content/drive/MyDrive/ssmt/my_checkpoint_word2vec.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer=None):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "Ex1KgUjcFM94"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model_word2vec.train()\n",
        "    losses = []\n",
        "    loop = tqdm(enumerate(train_iter), total=len(train_iter))\n",
        "    for batch_idx, batch in loop:\n",
        "        # Get input and targets and move to GPU if available\n",
        "        # Switching axis because bucket-iterator gives output of size(seq_len,bs)\n",
        "        inp_data = batch.eng_text.permute(-1, -2).to(device)\n",
        "        target = batch.hindi_text.permute(-1, -2).to(device)\n",
        "        # print(inp_data)\n",
        "        # print(\"=\"*80)\n",
        "        # print(target)\n",
        "\n",
        "        # Forward prop\n",
        "        output = model_word2vec(inp_data, target[:, :-1])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output.reshape(-1, trg_vocab_size), target[:, 1:].reshape(-1))\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # # Checking GPU uses\n",
        "        # if device.type == \"cuda\":\n",
        "        #     total_mem = torch.cuda.get_device_properties(0).total_memory/1024/1024\n",
        "        #     allocated_mem = torch.cuda.memory_allocated(0)/1024/1024\n",
        "        #     reserved_mem = torch.cuda.memory_reserved(0)/1024/1024\n",
        "        # else:\n",
        "        #     total_mem = 0\n",
        "        #     allocated_mem = 0\n",
        "        #     reserved_mem = 0\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Clipping exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model_word2vec.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        # break\n",
        "    train_mean_loss = sum(losses) / len(losses)\n",
        "    scheduler.step(train_mean_loss)\n",
        "\n",
        "    model_word2vec.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for val_batch_idx, val_batch in tqdm(enumerate(val_iter), total=len(val_iter)):\n",
        "            val_inp_data = val_batch.eng_text.permute(-1, -2).to(device)\n",
        "            val_target = val_batch.hindi_text.permute(-1, -2).to(device)\n",
        "            val_output = model_word2vec(val_inp_data, val_target[:, :-1])\n",
        "            val_loss = criterion(val_output.reshape(-1, trg_vocab_size), val_target[:, 1:].reshape(-1))\n",
        "            val_losses.append(val_loss.item())\n",
        "        val_mean_loss = sum(val_losses)/len(val_losses)\n",
        "\n",
        "    loss_tracker.append(val_mean_loss)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        if save_model and val_mean_loss == np.min(loss_tracker):\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model_word2vec.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}]: train_loss= {train_mean_loss}; val_loss= {val_mean_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "wmFDN6RGE4Uu",
        "outputId": "a68ecbc4-ac6c-41d3-fd76-4d3801f027c2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/274 [00:26<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-030fbe69e39f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Gradient descent step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Update progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_graph_capture_health_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_built\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mcapturing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_current_stream_capturing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcapturing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'capturable'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\u001b[0m in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mIf\u001b[0m \u001b[0ma\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexist\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0minitializing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cuda_isCurrentStreamCapturing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RYSj2HiFTxz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}